{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset class\n",
    "class EnergyDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X = self.X.iloc[idx]\n",
    "        y = self.y.iloc[idx]\n",
    "        return X.values, y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "X_train = pd.read_csv('X_train.csv')\n",
    "y_train = pd.read_csv('y_train.csv')\n",
    "X_val = pd.read_csv('X_val.csv')\n",
    "y_val = pd.read_csv('y_val.csv')\n",
    "X_test = pd.read_csv('X_test.csv')\n",
    "y_test = pd.read_csv('y_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset\n",
    "train_dataset = EnergyDataset(X_train, y_train)\n",
    "val_dataset = EnergyDataset(X_val, y_val)\n",
    "test_dataset = EnergyDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loader\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_QCNN(nn.Module):\n",
    "    def __init__(self, input_size=24, hidden_size=128, seq_len=10):\n",
    "        super(RNN_QCNN, self).__init__()\n",
    "\n",
    "        # RNN Layer\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "\n",
    "        # Linear Layer after RNN to reshape data for QCNN\n",
    "        self.fc_rnn = nn.Linear(hidden_size, 256)\n",
    "\n",
    "        # QCNN Layers\n",
    "        self.fc1 = nn.Linear(256, 256)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "\n",
    "        self.fc4 = nn.Linear(64, 32)\n",
    "        self.bn4 = nn.BatchNorm1d(32)\n",
    "\n",
    "        self.fc5 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Pass through RNN\n",
    "        rnn_out, _ = self.rnn(x)\n",
    "\n",
    "        # Handle case where sequence length might be 1\n",
    "        if rnn_out.dim() == 3:\n",
    "            # Take the last output of RNN for each batch (dim=1)\n",
    "            rnn_out_last = rnn_out[:, -1, :]  # Last time step output\n",
    "        else:\n",
    "            rnn_out_last = rnn_out  # If no sequence length, directly use output\n",
    "\n",
    "        # Pass through Linear layer to reshape data for QCNN\n",
    "        x = self.fc_rnn(rnn_out_last)\n",
    "\n",
    "        # QCNN Layers\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.fc4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        # Output layer\n",
    "        x = self.fc5(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters\n",
    "learning_rate = 0.001\n",
    "n_epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model, optimizer, and loss function\n",
    "model = RNN_QCNN()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists to log training and validation loss and accuracy\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Train Loss: 0.007794812729799539\n",
      "Epoch [2/200], Train Loss: 0.002509972009631153\n",
      "Epoch [3/200], Train Loss: 0.002340654270458454\n",
      "Epoch [4/200], Train Loss: 0.002024359110897016\n",
      "Epoch [5/200], Train Loss: 0.002106437262463671\n",
      "Epoch [6/200], Train Loss: 0.0020555776480243528\n",
      "Epoch [7/200], Train Loss: 0.002034321620940769\n",
      "Epoch [8/200], Train Loss: 0.00187443094997396\n",
      "Epoch [9/200], Train Loss: 0.002017519344494349\n",
      "Epoch [10/200], Train Loss: 0.0018684194105506706\n",
      "Epoch [11/200], Train Loss: 0.0018874592719371327\n",
      "Epoch [12/200], Train Loss: 0.0018324625962644553\n",
      "Epoch [13/200], Train Loss: 0.001799427328183227\n",
      "Epoch [14/200], Train Loss: 0.0018228527955420077\n",
      "Epoch [15/200], Train Loss: 0.001691327332144098\n",
      "Epoch [16/200], Train Loss: 0.0016240321768045843\n",
      "Epoch [17/200], Train Loss: 0.0016136439667402667\n",
      "Epoch [18/200], Train Loss: 0.0015940944676095016\n",
      "Epoch [19/200], Train Loss: 0.001571750298582292\n",
      "Epoch [20/200], Train Loss: 0.0014987360756489724\n",
      "Epoch [21/200], Train Loss: 0.001505797611949589\n",
      "Epoch [22/200], Train Loss: 0.0014815256676504285\n",
      "Epoch [23/200], Train Loss: 0.0013913374189351172\n",
      "Epoch [24/200], Train Loss: 0.001328994932281208\n",
      "Epoch [25/200], Train Loss: 0.0013060253633355\n",
      "Epoch [26/200], Train Loss: 0.001294027289308576\n",
      "Epoch [27/200], Train Loss: 0.001292146061648186\n",
      "Epoch [28/200], Train Loss: 0.0012249152762408586\n",
      "Epoch [29/200], Train Loss: 0.0011748257822245104\n",
      "Epoch [30/200], Train Loss: 0.0012186382972291395\n",
      "Epoch [31/200], Train Loss: 0.0011732211768345766\n",
      "Epoch [32/200], Train Loss: 0.0011048066163268013\n",
      "Epoch [33/200], Train Loss: 0.001041660587845347\n",
      "Epoch [34/200], Train Loss: 0.0011344215004462652\n",
      "Epoch [35/200], Train Loss: 0.0010001966663356384\n",
      "Epoch [36/200], Train Loss: 0.000930211599464674\n",
      "Epoch [37/200], Train Loss: 0.0008924357343608977\n",
      "Epoch [38/200], Train Loss: 0.0008556829696417041\n",
      "Epoch [39/200], Train Loss: 0.0007771106874810063\n",
      "Epoch [40/200], Train Loss: 0.0007422118033513165\n",
      "Epoch [41/200], Train Loss: 0.0007532064915944931\n",
      "Epoch [42/200], Train Loss: 0.000695567698989123\n",
      "Epoch [43/200], Train Loss: 0.0007054629244298566\n",
      "Epoch [44/200], Train Loss: 0.0006840024345135195\n",
      "Epoch [45/200], Train Loss: 0.0006550449862306078\n",
      "Epoch [46/200], Train Loss: 0.0006766080399047804\n",
      "Epoch [47/200], Train Loss: 0.000633197914004706\n",
      "Epoch [48/200], Train Loss: 0.00064616045400663\n",
      "Epoch [49/200], Train Loss: 0.0006240954716312559\n",
      "Epoch [50/200], Train Loss: 0.0006135140660336069\n",
      "Epoch [51/200], Train Loss: 0.0006339571954136971\n",
      "Epoch [52/200], Train Loss: 0.0006031059988374722\n",
      "Epoch [53/200], Train Loss: 0.0006551776613350751\n",
      "Epoch [54/200], Train Loss: 0.0006229568096877997\n",
      "Epoch [55/200], Train Loss: 0.0005998368851554846\n",
      "Epoch [56/200], Train Loss: 0.0005935193712621529\n",
      "Epoch [57/200], Train Loss: 0.0006007634008359747\n",
      "Epoch [58/200], Train Loss: 0.0005936441900778679\n",
      "Epoch [59/200], Train Loss: 0.000581021475677914\n",
      "Epoch [60/200], Train Loss: 0.0005696825888778922\n",
      "Epoch [61/200], Train Loss: 0.000576322016824854\n",
      "Epoch [62/200], Train Loss: 0.0005700017488440147\n",
      "Epoch [63/200], Train Loss: 0.0005574907778991165\n",
      "Epoch [64/200], Train Loss: 0.0005731860547134655\n",
      "Epoch [65/200], Train Loss: 0.0005975535757072584\n",
      "Epoch [66/200], Train Loss: 0.0005573082561861187\n",
      "Epoch [67/200], Train Loss: 0.0005520364261493985\n",
      "Epoch [68/200], Train Loss: 0.0005730187422372801\n",
      "Epoch [69/200], Train Loss: 0.0005564057435577789\n",
      "Epoch [70/200], Train Loss: 0.0005428842465818837\n",
      "Epoch [71/200], Train Loss: 0.0005530263795857373\n",
      "Epoch [72/200], Train Loss: 0.0005416562027339501\n",
      "Epoch [73/200], Train Loss: 0.0005405759580217538\n",
      "Epoch [74/200], Train Loss: 0.0005450443000369674\n",
      "Epoch [75/200], Train Loss: 0.0005352744385680534\n",
      "Epoch [76/200], Train Loss: 0.0005490417177146884\n",
      "Epoch [77/200], Train Loss: 0.0005492938577253347\n",
      "Epoch [78/200], Train Loss: 0.0005304792123466349\n",
      "Epoch [79/200], Train Loss: 0.0005530138767205696\n",
      "Epoch [80/200], Train Loss: 0.0005113407725678038\n",
      "Epoch [81/200], Train Loss: 0.0005513256571071667\n",
      "Epoch [82/200], Train Loss: 0.0005331593827842923\n",
      "Epoch [83/200], Train Loss: 0.0005274837331347304\n",
      "Epoch [84/200], Train Loss: 0.000540757039095144\n",
      "Epoch [85/200], Train Loss: 0.0005353200007853859\n",
      "Epoch [86/200], Train Loss: 0.0005349976518438799\n",
      "Epoch [87/200], Train Loss: 0.0005348073579093217\n",
      "Epoch [88/200], Train Loss: 0.0005095116393789382\n",
      "Epoch [89/200], Train Loss: 0.0005336598468456609\n",
      "Epoch [90/200], Train Loss: 0.0005188411503498694\n",
      "Epoch [91/200], Train Loss: 0.0005310923473152418\n",
      "Epoch [92/200], Train Loss: 0.0005218659266082732\n",
      "Epoch [93/200], Train Loss: 0.0005103726873588366\n",
      "Epoch [94/200], Train Loss: 0.0005066597619953941\n",
      "Epoch [95/200], Train Loss: 0.0005234056865390283\n",
      "Epoch [96/200], Train Loss: 0.000524840309128901\n",
      "Epoch [97/200], Train Loss: 0.0005018186885326242\n",
      "Epoch [98/200], Train Loss: 0.0005262564253253253\n",
      "Epoch [99/200], Train Loss: 0.0005307042850170836\n",
      "Epoch [100/200], Train Loss: 0.0004960804195443807\n",
      "Epoch [101/200], Train Loss: 0.000513062025692813\n",
      "Epoch [102/200], Train Loss: 0.0005044742059768536\n",
      "Epoch [103/200], Train Loss: 0.0004941127015228566\n",
      "Epoch [104/200], Train Loss: 0.0005163808155437868\n",
      "Epoch [105/200], Train Loss: 0.0005100131756337687\n",
      "Epoch [106/200], Train Loss: 0.000551164476546238\n",
      "Epoch [107/200], Train Loss: 0.0005006487426370646\n",
      "Epoch [108/200], Train Loss: 0.0005054984365159165\n",
      "Epoch [109/200], Train Loss: 0.0005111264122508568\n",
      "Epoch [110/200], Train Loss: 0.0005226086063120475\n",
      "Epoch [111/200], Train Loss: 0.0004896181975925436\n",
      "Epoch [112/200], Train Loss: 0.0005193757731824168\n",
      "Epoch [113/200], Train Loss: 0.0004979273263134932\n",
      "Epoch [114/200], Train Loss: 0.0005045743943954001\n",
      "Epoch [115/200], Train Loss: 0.0004938895423628567\n",
      "Epoch [116/200], Train Loss: 0.0004935267245071555\n",
      "Epoch [117/200], Train Loss: 0.0005001970648675558\n",
      "Epoch [118/200], Train Loss: 0.0004998355146316566\n",
      "Epoch [119/200], Train Loss: 0.0005229473245075771\n",
      "Epoch [120/200], Train Loss: 0.000487775902831933\n",
      "Epoch [121/200], Train Loss: 0.0005307281043584188\n",
      "Epoch [122/200], Train Loss: 0.0004984557232219966\n",
      "Epoch [123/200], Train Loss: 0.0005134581990236164\n",
      "Epoch [124/200], Train Loss: 0.0004782332367537849\n",
      "Epoch [125/200], Train Loss: 0.0004920272170996319\n",
      "Epoch [126/200], Train Loss: 0.00048524410296304365\n",
      "Epoch [127/200], Train Loss: 0.0005064602647618132\n",
      "Epoch [128/200], Train Loss: 0.000505284963922716\n",
      "Epoch [129/200], Train Loss: 0.00048402694969637154\n",
      "Epoch [130/200], Train Loss: 0.00047719939883917966\n",
      "Epoch [131/200], Train Loss: 0.0005071065439466227\n",
      "Epoch [132/200], Train Loss: 0.0005037951644165793\n",
      "Epoch [133/200], Train Loss: 0.0005379475509284023\n",
      "Epoch [134/200], Train Loss: 0.0004953523049607018\n",
      "Epoch [135/200], Train Loss: 0.00047646859408637956\n",
      "Epoch [136/200], Train Loss: 0.00048121598607546624\n",
      "Epoch [137/200], Train Loss: 0.0005259262406422952\n",
      "Epoch [138/200], Train Loss: 0.0005055009839927895\n",
      "Epoch [139/200], Train Loss: 0.0004796573167561761\n",
      "Epoch [140/200], Train Loss: 0.000489880150583824\n",
      "Epoch [141/200], Train Loss: 0.0005124932281142332\n",
      "Epoch [142/200], Train Loss: 0.0004949561911087175\n",
      "Epoch [143/200], Train Loss: 0.0004905090077869434\n",
      "Epoch [144/200], Train Loss: 0.00047490183775364197\n",
      "Epoch [145/200], Train Loss: 0.00047359360446695074\n",
      "Epoch [146/200], Train Loss: 0.0005016061784148597\n",
      "Epoch [147/200], Train Loss: 0.0004747313672272644\n",
      "Epoch [148/200], Train Loss: 0.0004983854657120249\n",
      "Epoch [149/200], Train Loss: 0.0005113840308583754\n",
      "Epoch [150/200], Train Loss: 0.000473639487073061\n",
      "Epoch [151/200], Train Loss: 0.0004941158026744912\n",
      "Epoch [152/200], Train Loss: 0.0004894145980659165\n",
      "Epoch [153/200], Train Loss: 0.0004923109952981172\n",
      "Epoch [154/200], Train Loss: 0.000500656561844046\n",
      "Epoch [155/200], Train Loss: 0.0004957166145110353\n",
      "Epoch [156/200], Train Loss: 0.00047459292778095067\n",
      "Epoch [157/200], Train Loss: 0.0004589415895354945\n",
      "Epoch [158/200], Train Loss: 0.0004965810613402238\n",
      "Epoch [159/200], Train Loss: 0.0005135905317406806\n",
      "Epoch [160/200], Train Loss: 0.0005121559761142822\n",
      "Epoch [161/200], Train Loss: 0.00047158599382122514\n",
      "Epoch [162/200], Train Loss: 0.00047632761928662683\n",
      "Epoch [163/200], Train Loss: 0.0004983557794349951\n",
      "Epoch [164/200], Train Loss: 0.0004959891381880846\n",
      "Epoch [165/200], Train Loss: 0.0004905965193813774\n",
      "Epoch [166/200], Train Loss: 0.0004879258039510339\n",
      "Epoch [167/200], Train Loss: 0.0004809061364133234\n",
      "Epoch [168/200], Train Loss: 0.00046568934279317924\n",
      "Epoch [169/200], Train Loss: 0.0004788788321124137\n",
      "Epoch [170/200], Train Loss: 0.0004777511280646846\n",
      "Epoch [171/200], Train Loss: 0.0005022472530227601\n",
      "Epoch [172/200], Train Loss: 0.0004936799933796024\n",
      "Epoch [173/200], Train Loss: 0.0004805739565052437\n",
      "Epoch [174/200], Train Loss: 0.0005086434152368286\n",
      "Epoch [175/200], Train Loss: 0.00047313369037034025\n",
      "Epoch [176/200], Train Loss: 0.00048499354288817384\n",
      "Epoch [177/200], Train Loss: 0.000491714623490788\n",
      "Epoch [178/200], Train Loss: 0.0005141445375440735\n",
      "Epoch [179/200], Train Loss: 0.0004729417073321826\n",
      "Epoch [180/200], Train Loss: 0.0004773868542206152\n",
      "Epoch [181/200], Train Loss: 0.0004789168179614076\n",
      "Epoch [182/200], Train Loss: 0.0004892658017729949\n",
      "Epoch [183/200], Train Loss: 0.0004844179605502657\n",
      "Epoch [184/200], Train Loss: 0.00046510624692388576\n",
      "Epoch [185/200], Train Loss: 0.0004967815887431567\n",
      "Epoch [186/200], Train Loss: 0.00048439431321370684\n",
      "Epoch [187/200], Train Loss: 0.0004724349979378922\n",
      "Epoch [188/200], Train Loss: 0.0004919059960694615\n",
      "Epoch [189/200], Train Loss: 0.0004900054772082575\n",
      "Epoch [190/200], Train Loss: 0.00047413142441408865\n",
      "Epoch [191/200], Train Loss: 0.0004763645331888014\n",
      "Epoch [192/200], Train Loss: 0.00047271069404629645\n",
      "Epoch [193/200], Train Loss: 0.0005089504133496796\n",
      "Epoch [194/200], Train Loss: 0.00048256368064634244\n",
      "Epoch [195/200], Train Loss: 0.0004830085968978477\n",
      "Epoch [196/200], Train Loss: 0.0004866635633679289\n",
      "Epoch [197/200], Train Loss: 0.0004725234148815345\n",
      "Epoch [198/200], Train Loss: 0.00047943663035930444\n",
      "Epoch [199/200], Train Loss: 0.0004822894996256753\n",
      "Epoch [200/200], Train Loss: 0.0005013748138908945\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    epoch_loss = 0\n",
    "    for batch in train_loader:\n",
    "        X, y = batch\n",
    "        X = X.clone().detach().float()\n",
    "        y = y.clone().detach().float()  \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X)\n",
    "        loss = loss_fn(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    # Calculate average loss for the epoch\n",
    "    average_loss = epoch_loss / len(train_loader)\n",
    "    train_losses.append(average_loss)\n",
    "\n",
    "    # Calculate training accuracy (R² score)\n",
    "    with torch.no_grad():\n",
    "        y_train_pred = model(torch.tensor(X_train.values, dtype=torch.float32)).flatten().numpy()\n",
    "        r2 = r2_score(y_train.values, y_train_pred)\n",
    "        accuracy = r2 * 100\n",
    "        train_accuracies.append(accuracy)\n",
    "\n",
    "    # Validate the model\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            X, y = batch\n",
    "            X = X.clone().detach().float()\n",
    "            y = y.clone().detach().float()  \n",
    "            outputs = model(X)\n",
    "            loss = loss_fn(outputs, y)\n",
    "            val_loss += loss.item()\n",
    "        \n",
    "        average_val_loss = val_loss / len(val_loader)\n",
    "        val_losses.append(average_val_loss)\n",
    "\n",
    "        y_val_pred = model(torch.tensor(X_val.values, dtype=torch.float32)).flatten().numpy()\n",
    "        val_r2 = r2_score(y_val.values, y_val_pred)\n",
    "        val_accuracy = val_r2 * 100\n",
    "        val_accuracies.append(val_accuracy)\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{n_epochs}], Train Loss: {average_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained model and metrics\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'train_losses': train_losses,\n",
    "    'train_accuracies': train_accuracies,\n",
    "    'val_losses': val_losses,\n",
    "    'val_accuracies': val_accuracies\n",
    "}, 'models/qcnn_rnn_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error on test data: 0.0003835035616209055\n",
      "R² score (Accuracy) on test data: 0.9902209995058393\n",
      "Accuracy: 99.02209995058394%\n",
      "Mean Absolute Error (MAE) on test data: 0.01363999795374514\n",
      "Root Mean Squared Error (RMSE) on test data: 0.019583246963180173\n",
      "Mean Absolute Percentage Error (MAPE) on test data: 87.99191732960747%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "# Test the model on the test set\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_test_pred = model(X_test_tensor).flatten().numpy()\n",
    "    \n",
    "    # Calculate MSE, MAE, RMSE, R² score, and MAPE\n",
    "    test_mse = mean_squared_error(y_test.values, y_test_pred)\n",
    "    test_r2 = r2_score(y_test.values, y_test_pred)\n",
    "    test_mae = mean_absolute_error(y_test.values, y_test_pred)\n",
    "    test_rmse = mean_squared_error(y_test.values, y_test_pred, squared=False)\n",
    "    test_mape = np.mean(np.abs((y_test.values - y_test_pred) / y_test.values)) * 100\n",
    "\n",
    "    # Calculate accuracy as R² score in percentage\n",
    "    test_accuracy = test_r2 * 100\n",
    "    \n",
    "    print(f'Mean Squared Error on test data: {test_mse}')\n",
    "    print(f'R² score (Accuracy) on test data: {test_r2}')\n",
    "    print(f'Accuracy: {test_accuracy}%')\n",
    "    print(f'Mean Absolute Error (MAE) on test data: {test_mae}')\n",
    "    print(f'Root Mean Squared Error (RMSE) on test data: {test_rmse}')\n",
    "    print(f'Mean Absolute Percentage Error (MAPE) on test data: {test_mape}%')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
