{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset class\n",
    "class EnergyDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X = self.X.iloc[idx]\n",
    "        y = self.y.iloc[idx]\n",
    "        return X.values.reshape(1, -1), y.values  # Adding an extra dimension for sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "X_train = pd.read_csv('X_train.csv')\n",
    "y_train = pd.read_csv('y_train.csv')\n",
    "X_val = pd.read_csv('X_val.csv')\n",
    "y_val = pd.read_csv('y_val.csv')\n",
    "X_test = pd.read_csv('X_test.csv')\n",
    "y_test = pd.read_csv('y_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset\n",
    "train_dataset = EnergyDataset(X_train, y_train)\n",
    "val_dataset = EnergyDataset(X_val, y_val)\n",
    "test_dataset = EnergyDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loader\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Define the LSTM-QCNN model\n",
    "class LSTM_QCNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, lstm_layers, fc_dim):\n",
    "        super(LSTM_QCNN, self).__init__()\n",
    "        \n",
    "        # Define LSTM layer(s)\n",
    "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_dim, \n",
    "                            num_layers=lstm_layers, batch_first=True, dropout=0.3)\n",
    "        \n",
    "        # Define QCNN layers\n",
    "        self.fc1 = nn.Linear(hidden_dim, 256)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "        self.fc4 = nn.Linear(64, 32)\n",
    "        self.bn4 = nn.BatchNorm1d(32)\n",
    "        self.fc5 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # LSTM layer: process sequential data\n",
    "        x, _ = self.lstm(x)\n",
    "        x = x[:, -1, :]  # Use the last output of the LSTM\n",
    "        \n",
    "        # QCNN fully connected layers\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc5(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters\n",
    "learning_rate = 0.001\n",
    "n_epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters for the LSTM-QCNN model\n",
    "input_dim = X_train.shape[1]   # Number of features in input\n",
    "hidden_dim = 128               # Hidden dimension for the LSTM layer\n",
    "lstm_layers = 2                # Number of LSTM layers\n",
    "fc_dim = 256                   # Dimension for fully connected layers in QCNN part\n",
    "\n",
    "# Initialize model, optimizer, and loss function\n",
    "model = LSTM_QCNN(input_dim=input_dim, hidden_dim=hidden_dim, lstm_layers=lstm_layers, fc_dim=fc_dim)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists to log training and validation loss and accuracy\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Train Loss: 0.029633477690381955\n",
      "Epoch [2/200], Train Loss: 0.0036036595932118147\n",
      "Epoch [3/200], Train Loss: 0.002991866680166309\n",
      "Epoch [4/200], Train Loss: 0.0027298397829906705\n",
      "Epoch [5/200], Train Loss: 0.0024894981813068213\n",
      "Epoch [6/200], Train Loss: 0.0023090533423905336\n",
      "Epoch [7/200], Train Loss: 0.002355882569179461\n",
      "Epoch [8/200], Train Loss: 0.0022782519776337676\n",
      "Epoch [9/200], Train Loss: 0.002386213583099769\n",
      "Epoch [10/200], Train Loss: 0.0022089677181558118\n",
      "Epoch [11/200], Train Loss: 0.0022174328077776357\n",
      "Epoch [12/200], Train Loss: 0.0021137453542355656\n",
      "Epoch [13/200], Train Loss: 0.0019390044819713587\n",
      "Epoch [14/200], Train Loss: 0.0017485499499724495\n",
      "Epoch [15/200], Train Loss: 0.0013052824350159537\n",
      "Epoch [16/200], Train Loss: 0.001234400579769582\n",
      "Epoch [17/200], Train Loss: 0.0011312478188409518\n",
      "Epoch [18/200], Train Loss: 0.0009566111974460661\n",
      "Epoch [19/200], Train Loss: 0.0010621913768244225\n",
      "Epoch [20/200], Train Loss: 0.0009644203878528739\n",
      "Epoch [21/200], Train Loss: 0.0008764524846494069\n",
      "Epoch [22/200], Train Loss: 0.0008377924359367427\n",
      "Epoch [23/200], Train Loss: 0.0008039734660370962\n",
      "Epoch [24/200], Train Loss: 0.0008222008672071745\n",
      "Epoch [25/200], Train Loss: 0.0008516919159130381\n",
      "Epoch [26/200], Train Loss: 0.0007726910333092976\n",
      "Epoch [27/200], Train Loss: 0.0007885155786759848\n",
      "Epoch [28/200], Train Loss: 0.0007355827766276636\n",
      "Epoch [29/200], Train Loss: 0.0007265691341010543\n",
      "Epoch [30/200], Train Loss: 0.0007180623011625611\n",
      "Epoch [31/200], Train Loss: 0.0007137703103929481\n",
      "Epoch [32/200], Train Loss: 0.0006872669666473702\n",
      "Epoch [33/200], Train Loss: 0.0006847722668199517\n",
      "Epoch [34/200], Train Loss: 0.0006838555542543905\n",
      "Epoch [35/200], Train Loss: 0.0006982883945944679\n",
      "Epoch [36/200], Train Loss: 0.0006763504862970814\n",
      "Epoch [37/200], Train Loss: 0.0006662952528060281\n",
      "Epoch [38/200], Train Loss: 0.0006690881597673498\n",
      "Epoch [39/200], Train Loss: 0.0006580945880943363\n",
      "Epoch [40/200], Train Loss: 0.0006227653259302172\n",
      "Epoch [41/200], Train Loss: 0.0006480701647548921\n",
      "Epoch [42/200], Train Loss: 0.0006174629660702105\n",
      "Epoch [43/200], Train Loss: 0.0006414953073554379\n",
      "Epoch [44/200], Train Loss: 0.0006173131305959945\n",
      "Epoch [45/200], Train Loss: 0.0006325281145801826\n",
      "Epoch [46/200], Train Loss: 0.000610652832963839\n",
      "Epoch [47/200], Train Loss: 0.0006282515509846053\n",
      "Epoch [48/200], Train Loss: 0.000601722478314553\n",
      "Epoch [49/200], Train Loss: 0.0005989646599576426\n",
      "Epoch [50/200], Train Loss: 0.0006349187325074543\n",
      "Epoch [51/200], Train Loss: 0.0005986622683892106\n",
      "Epoch [52/200], Train Loss: 0.0005901942140660449\n",
      "Epoch [53/200], Train Loss: 0.000609714741206326\n",
      "Epoch [54/200], Train Loss: 0.0005942064606007237\n",
      "Epoch [55/200], Train Loss: 0.0006081773423095874\n",
      "Epoch [56/200], Train Loss: 0.0005882444584202154\n",
      "Epoch [57/200], Train Loss: 0.0005819648399528947\n",
      "Epoch [58/200], Train Loss: 0.0005773526367930528\n",
      "Epoch [59/200], Train Loss: 0.0005692620162540655\n",
      "Epoch [60/200], Train Loss: 0.000580780292196974\n",
      "Epoch [61/200], Train Loss: 0.0005667098208040368\n",
      "Epoch [62/200], Train Loss: 0.0005515331554087243\n",
      "Epoch [63/200], Train Loss: 0.0005674969819561072\n",
      "Epoch [64/200], Train Loss: 0.0005666623612598066\n",
      "Epoch [65/200], Train Loss: 0.0005832587482924956\n",
      "Epoch [66/200], Train Loss: 0.0005655367746011195\n",
      "Epoch [67/200], Train Loss: 0.0005607577546788355\n",
      "Epoch [68/200], Train Loss: 0.0005512489074414032\n",
      "Epoch [69/200], Train Loss: 0.0005517901870826711\n",
      "Epoch [70/200], Train Loss: 0.0005569294921064511\n",
      "Epoch [71/200], Train Loss: 0.0005549780618330474\n",
      "Epoch [72/200], Train Loss: 0.0005488817216644812\n",
      "Epoch [73/200], Train Loss: 0.0005497362906460236\n",
      "Epoch [74/200], Train Loss: 0.0005701156052295726\n",
      "Epoch [75/200], Train Loss: 0.000541961804526977\n",
      "Epoch [76/200], Train Loss: 0.0005382854969617443\n",
      "Epoch [77/200], Train Loss: 0.000548361045723302\n",
      "Epoch [78/200], Train Loss: 0.0005446431790929314\n",
      "Epoch [79/200], Train Loss: 0.0005368643111588477\n",
      "Epoch [80/200], Train Loss: 0.0005462976312052308\n",
      "Epoch [81/200], Train Loss: 0.0005263240907445667\n",
      "Epoch [82/200], Train Loss: 0.0005302059032266408\n",
      "Epoch [83/200], Train Loss: 0.0005230995711127224\n",
      "Epoch [84/200], Train Loss: 0.0005408405229395238\n",
      "Epoch [85/200], Train Loss: 0.0005392918187793711\n",
      "Epoch [86/200], Train Loss: 0.0005531908176317835\n",
      "Epoch [87/200], Train Loss: 0.0005285014755686324\n",
      "Epoch [88/200], Train Loss: 0.0005242186738876322\n",
      "Epoch [89/200], Train Loss: 0.0005240460259431908\n",
      "Epoch [90/200], Train Loss: 0.0005367558316205904\n",
      "Epoch [91/200], Train Loss: 0.0005349566363696145\n",
      "Epoch [92/200], Train Loss: 0.0005386272628633476\n",
      "Epoch [93/200], Train Loss: 0.0005399105696109606\n",
      "Epoch [94/200], Train Loss: 0.0005305183301524975\n",
      "Epoch [95/200], Train Loss: 0.0005240908422373172\n",
      "Epoch [96/200], Train Loss: 0.0005179389774053499\n",
      "Epoch [97/200], Train Loss: 0.0005377122175640268\n",
      "Epoch [98/200], Train Loss: 0.0005071969996572253\n",
      "Epoch [99/200], Train Loss: 0.0005277284571896107\n",
      "Epoch [100/200], Train Loss: 0.0005301806998027506\n",
      "Epoch [101/200], Train Loss: 0.0005151681541327103\n",
      "Epoch [102/200], Train Loss: 0.0005131009771287346\n",
      "Epoch [103/200], Train Loss: 0.0005318027506623419\n",
      "Epoch [104/200], Train Loss: 0.0005191532981901028\n",
      "Epoch [105/200], Train Loss: 0.0005101523425999539\n",
      "Epoch [106/200], Train Loss: 0.0005226875888904669\n",
      "Epoch [107/200], Train Loss: 0.0005073125887641833\n",
      "Epoch [108/200], Train Loss: 0.0005225691839215082\n",
      "Epoch [109/200], Train Loss: 0.00053346754209065\n",
      "Epoch [110/200], Train Loss: 0.0005326911355059161\n",
      "Epoch [111/200], Train Loss: 0.000501432645023373\n",
      "Epoch [112/200], Train Loss: 0.0005296596001390176\n",
      "Epoch [113/200], Train Loss: 0.0005166684296992219\n",
      "Epoch [114/200], Train Loss: 0.0005064088069128645\n",
      "Epoch [115/200], Train Loss: 0.000522843400041319\n",
      "Epoch [116/200], Train Loss: 0.0005130869009112515\n",
      "Epoch [117/200], Train Loss: 0.0005231338623784255\n",
      "Epoch [118/200], Train Loss: 0.000511828403775137\n",
      "Epoch [119/200], Train Loss: 0.0005075330681330252\n",
      "Epoch [120/200], Train Loss: 0.0005254948620356297\n",
      "Epoch [121/200], Train Loss: 0.0005213033694297627\n",
      "Epoch [122/200], Train Loss: 0.0005099353135717945\n",
      "Epoch [123/200], Train Loss: 0.0005354634154309619\n",
      "Epoch [124/200], Train Loss: 0.0005179909047670055\n",
      "Epoch [125/200], Train Loss: 0.0005051029909516271\n",
      "Epoch [126/200], Train Loss: 0.000506029555277633\n",
      "Epoch [127/200], Train Loss: 0.0005132162407622824\n",
      "Epoch [128/200], Train Loss: 0.0004981596075322277\n",
      "Epoch [129/200], Train Loss: 0.0005067314332134954\n",
      "Epoch [130/200], Train Loss: 0.0005218469788762953\n",
      "Epoch [131/200], Train Loss: 0.0005065546634153498\n",
      "Epoch [132/200], Train Loss: 0.0005210702376354563\n",
      "Epoch [133/200], Train Loss: 0.0005034065810953494\n",
      "Epoch [134/200], Train Loss: 0.0004932594228048069\n",
      "Epoch [135/200], Train Loss: 0.0005094385369790256\n",
      "Epoch [136/200], Train Loss: 0.0005093449954940079\n",
      "Epoch [137/200], Train Loss: 0.0005113861757955019\n",
      "Epoch [138/200], Train Loss: 0.0004969465411149786\n",
      "Epoch [139/200], Train Loss: 0.0005058420381596496\n",
      "Epoch [140/200], Train Loss: 0.0004972938628125975\n",
      "Epoch [141/200], Train Loss: 0.0005134408247957341\n",
      "Epoch [142/200], Train Loss: 0.0004882554711778241\n",
      "Epoch [143/200], Train Loss: 0.0004902760097867782\n",
      "Epoch [144/200], Train Loss: 0.000497123753086444\n",
      "Epoch [145/200], Train Loss: 0.0004905051683959693\n",
      "Epoch [146/200], Train Loss: 0.0004919298248021891\n",
      "Epoch [147/200], Train Loss: 0.0004911860223242411\n",
      "Epoch [148/200], Train Loss: 0.0005028309342319344\n",
      "Epoch [149/200], Train Loss: 0.0005014005177716556\n",
      "Epoch [150/200], Train Loss: 0.0005190149553328378\n",
      "Epoch [151/200], Train Loss: 0.0004931401696508129\n",
      "Epoch [152/200], Train Loss: 0.0004895461903143714\n",
      "Epoch [153/200], Train Loss: 0.0005013468189467342\n",
      "Epoch [154/200], Train Loss: 0.0004987107009320025\n",
      "Epoch [155/200], Train Loss: 0.000495078740356887\n",
      "Epoch [156/200], Train Loss: 0.0005083724192262905\n",
      "Epoch [157/200], Train Loss: 0.0004965576414461156\n",
      "Epoch [158/200], Train Loss: 0.0004938560265320993\n",
      "Epoch [159/200], Train Loss: 0.00047807739223028757\n",
      "Epoch [160/200], Train Loss: 0.0005005068292956972\n",
      "Epoch [161/200], Train Loss: 0.0004850460538157806\n",
      "Epoch [162/200], Train Loss: 0.0004941539094542643\n",
      "Epoch [163/200], Train Loss: 0.0005069454251981318\n",
      "Epoch [164/200], Train Loss: 0.000495771591494988\n",
      "Epoch [165/200], Train Loss: 0.0004917598072444896\n",
      "Epoch [166/200], Train Loss: 0.00048192271691379094\n",
      "Epoch [167/200], Train Loss: 0.0004958580152246666\n",
      "Epoch [168/200], Train Loss: 0.00048177020921844825\n",
      "Epoch [169/200], Train Loss: 0.00048557423875391525\n",
      "Epoch [170/200], Train Loss: 0.0004963480458853492\n",
      "Epoch [171/200], Train Loss: 0.0004965939572209375\n",
      "Epoch [172/200], Train Loss: 0.0004967989578674337\n",
      "Epoch [173/200], Train Loss: 0.0005013402979622135\n",
      "Epoch [174/200], Train Loss: 0.0004814951571413848\n",
      "Epoch [175/200], Train Loss: 0.0005004405036581998\n",
      "Epoch [176/200], Train Loss: 0.0004874630306402374\n",
      "Epoch [177/200], Train Loss: 0.0004908334494342559\n",
      "Epoch [178/200], Train Loss: 0.0004938468898076144\n",
      "Epoch [179/200], Train Loss: 0.00048263467281933563\n",
      "Epoch [180/200], Train Loss: 0.0004909340693256266\n",
      "Epoch [181/200], Train Loss: 0.000485574045329175\n",
      "Epoch [182/200], Train Loss: 0.0004873523537438081\n",
      "Epoch [183/200], Train Loss: 0.00048157319195754947\n",
      "Epoch [184/200], Train Loss: 0.00047741363685932666\n",
      "Epoch [185/200], Train Loss: 0.0004942348660274944\n",
      "Epoch [186/200], Train Loss: 0.0004902119181813148\n",
      "Epoch [187/200], Train Loss: 0.0004832780373522847\n",
      "Epoch [188/200], Train Loss: 0.00047724632089219944\n",
      "Epoch [189/200], Train Loss: 0.0004804902739427933\n",
      "Epoch [190/200], Train Loss: 0.00048183495338339566\n",
      "Epoch [191/200], Train Loss: 0.0004809019644342477\n",
      "Epoch [192/200], Train Loss: 0.0004838208451723216\n",
      "Epoch [193/200], Train Loss: 0.0004894143110497841\n",
      "Epoch [194/200], Train Loss: 0.000497294521120752\n",
      "Epoch [195/200], Train Loss: 0.0004814668249893166\n",
      "Epoch [196/200], Train Loss: 0.0004805098862628267\n",
      "Epoch [197/200], Train Loss: 0.000494133981166048\n",
      "Epoch [198/200], Train Loss: 0.00048507082727516123\n",
      "Epoch [199/200], Train Loss: 0.00047354099072731187\n",
      "Epoch [200/200], Train Loss: 0.0004968483972689804\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()  \n",
    "    epoch_loss = 0\n",
    "    epoch_train_r2 = 0  \n",
    "\n",
    "    for batch in train_loader:\n",
    "        X, y = batch\n",
    "        X = X.clone().detach().float()  \n",
    "        y = y.clone().detach().float()  \n",
    "\n",
    "        optimizer.zero_grad()  \n",
    "        outputs = model(X)  \n",
    "        loss = loss_fn(outputs, y)\n",
    "        loss.backward()  \n",
    "        optimizer.step()  \n",
    "\n",
    "        # Accumulate the loss for this batch\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # Accumulate R² score for training accuracy\n",
    "        epoch_train_r2 += r2_score(y.numpy(), outputs.detach().numpy())\n",
    "\n",
    "    # Calculate the average loss and accuracy for this epoch\n",
    "    average_loss = epoch_loss / len(train_loader)\n",
    "    train_losses.append(average_loss)  \n",
    "    average_train_r2 = epoch_train_r2 / len(train_loader)  \n",
    "    train_accuracy = average_train_r2 * 100  \n",
    "    train_accuracies.append(train_accuracy)  \n",
    "\n",
    "    # Validate the model\n",
    "    model.eval()  \n",
    "    val_loss = 0  \n",
    "    epoch_val_r2 = 0  \n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            X, y = batch\n",
    "            X = X.clone().detach().float()\n",
    "            y = y.clone().detach().float()  \n",
    "\n",
    "            outputs = model(X)  \n",
    "            loss = loss_fn(outputs, y)  \n",
    "            val_loss += loss.item()  \n",
    "\n",
    "            # Accumulate R² score for validation accuracy\n",
    "            epoch_val_r2 += r2_score(y.numpy(), outputs.detach().numpy())\n",
    "\n",
    "        # Calculate the average validation loss and accuracy for this epoch\n",
    "        average_val_loss = val_loss / len(val_loader)\n",
    "        val_losses.append(average_val_loss)\n",
    "        average_val_r2 = epoch_val_r2 / len(val_loader)  \n",
    "        val_accuracy = average_val_r2 * 100  \n",
    "        val_accuracies.append(val_accuracy)\n",
    "    # Print training loss for this epoch\n",
    "    print(f'Epoch [{epoch + 1}/{n_epochs}], Train Loss: {average_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained model and metrics\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'train_losses': train_losses,\n",
    "    'train_accuracies': train_accuracies,\n",
    "    'val_losses': val_losses,\n",
    "    'val_accuracies': val_accuracies\n",
    "}, 'models/qcnn_lstm_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error on test data: 0.0004425968920501068\n",
      "R² score (Accuracy) on test data: 0.9887141720202578\n",
      "Accuracy: 98.87141720202578%\n",
      "Mean Absolute Error (MAE) on test data: 0.01543800679080574\n",
      "Root Mean Squared Error (RMSE) on test data: 0.02103798688206899\n",
      "Mean Absolute Percentage Error (MAPE) on test data: 86.92679167412251%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import numpy as np  \n",
    "\n",
    "# Test the model on the test set\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_test_pred = model(torch.tensor(X_test.values, dtype=torch.float32).unsqueeze(1)).flatten().numpy()\n",
    "    \n",
    "    # Calculate MSE, MAE, RMSE, R² score, and MAPE\n",
    "    test_mse = mean_squared_error(y_test.values, y_test_pred)\n",
    "    test_r2 = r2_score(y_test.values, y_test_pred)\n",
    "    test_mae = mean_absolute_error(y_test.values, y_test_pred)\n",
    "    test_rmse = mean_squared_error(y_test.values, y_test_pred, squared=False)\n",
    "    test_mape = np.mean(np.abs((y_test.values - y_test_pred) / y_test.values)) * 100\n",
    "\n",
    "    # Calculate accuracy as R² score in percentage\n",
    "    test_accuracy = test_r2 * 100\n",
    "    \n",
    "    print(f'Mean Squared Error on test data: {test_mse}')\n",
    "    print(f'R² score (Accuracy) on test data: {test_r2}')\n",
    "    print(f'Accuracy: {test_accuracy}%')\n",
    "    print(f'Mean Absolute Error (MAE) on test data: {test_mae}')\n",
    "    print(f'Root Mean Squared Error (RMSE) on test data: {test_rmse}')\n",
    "    print(f'Mean Absolute Percentage Error (MAPE) on test data: {test_mape}%')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
