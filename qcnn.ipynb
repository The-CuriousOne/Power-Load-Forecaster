{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset class\n",
    "class EnergyDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X = self.X.iloc[idx]\n",
    "        y = self.y.iloc[idx]\n",
    "        return X.values, y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "X_train = pd.read_csv('X_train.csv')\n",
    "y_train = pd.read_csv('y_train.csv')\n",
    "X_val = pd.read_csv('X_val.csv')\n",
    "y_val = pd.read_csv('y_val.csv')\n",
    "X_test = pd.read_csv('X_test.csv')\n",
    "y_test = pd.read_csv('y_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset\n",
    "train_dataset = EnergyDataset(X_train, y_train)\n",
    "val_dataset = EnergyDataset(X_val, y_val)\n",
    "test_dataset = EnergyDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loader\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the QCNN model with appropriate activation functions\n",
    "class QCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(QCNN, self).__init__()\n",
    "\n",
    "        # Input layer(24 features to 256 neurons)\n",
    "        self.fc1 = nn.Linear(24, 256)    \n",
    "        self.bn1 = nn.BatchNorm1d(256)    \n",
    "        self.relu = nn.ReLU()              \n",
    "        self.dropout = nn.Dropout(0.3)     \n",
    "        \n",
    "        # Hidden layer 1(256 to 128 neurons)\n",
    "        self.fc2 = nn.Linear(256, 128)    \n",
    "        self.bn2 = nn.BatchNorm1d(128)    \n",
    "        \n",
    "        # Hidden layer 2 (128 to 64 neurons)\n",
    "        self.fc3 = nn.Linear(128, 64)     \n",
    "        self.bn3 = nn.BatchNorm1d(64)      \n",
    "        \n",
    "        # Hidden layer 3 (64 to 32 neurons)\n",
    "        self.fc4 = nn.Linear(64, 32)      \n",
    "        self.bn4 = nn.BatchNorm1d(32)      \n",
    "        \n",
    "        # Output layer(32 neurons to 1 output)\n",
    "        self.fc5 = nn.Linear(32, 1)        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the network\n",
    "        \n",
    "        #Input Layer\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Hidden Layer \n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.fc4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        # Output Layer\n",
    "        x = self.fc5(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters\n",
    "learning_rate = 0.001\n",
    "n_epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model, optimizer, and loss function\n",
    "model = QCNN()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists to log training and validation loss and accuracy\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Train Loss: 0.008832881310918262\n",
      "Epoch [2/200], Train Loss: 0.003347938344237119\n",
      "Epoch [3/200], Train Loss: 0.002807968041878995\n",
      "Epoch [4/200], Train Loss: 0.0025850736627608993\n",
      "Epoch [5/200], Train Loss: 0.002453511380892541\n",
      "Epoch [6/200], Train Loss: 0.002301266304400326\n",
      "Epoch [7/200], Train Loss: 0.002059259650643825\n",
      "Epoch [8/200], Train Loss: 0.0019391761179980026\n",
      "Epoch [9/200], Train Loss: 0.0018715921610610795\n",
      "Epoch [10/200], Train Loss: 0.0017676994579721539\n",
      "Epoch [11/200], Train Loss: 0.001704873104199993\n",
      "Epoch [12/200], Train Loss: 0.0016378758009523153\n",
      "Epoch [13/200], Train Loss: 0.0016916817726690026\n",
      "Epoch [14/200], Train Loss: 0.0016171232352262142\n",
      "Epoch [15/200], Train Loss: 0.0015300253250288887\n",
      "Epoch [16/200], Train Loss: 0.0015074266755611622\n",
      "Epoch [17/200], Train Loss: 0.001411894126393015\n",
      "Epoch [18/200], Train Loss: 0.0013643376766904372\n",
      "Epoch [19/200], Train Loss: 0.0014510485970486815\n",
      "Epoch [20/200], Train Loss: 0.0012975824400560986\n",
      "Epoch [21/200], Train Loss: 0.0013532217975928676\n",
      "Epoch [22/200], Train Loss: 0.0012662161200145208\n",
      "Epoch [23/200], Train Loss: 0.0012635007566900503\n",
      "Epoch [24/200], Train Loss: 0.001246993465133854\n",
      "Epoch [25/200], Train Loss: 0.0013311603089862514\n",
      "Epoch [26/200], Train Loss: 0.0012472824440483427\n",
      "Epoch [27/200], Train Loss: 0.0011936243253303485\n",
      "Epoch [28/200], Train Loss: 0.0011780701542829894\n",
      "Epoch [29/200], Train Loss: 0.0011703975679218707\n",
      "Epoch [30/200], Train Loss: 0.001166575887823912\n",
      "Epoch [31/200], Train Loss: 0.0011434587241712633\n",
      "Epoch [32/200], Train Loss: 0.0010787167301912388\n",
      "Epoch [33/200], Train Loss: 0.0011422783789781046\n",
      "Epoch [34/200], Train Loss: 0.0011744820767062847\n",
      "Epoch [35/200], Train Loss: 0.0011347279955599289\n",
      "Epoch [36/200], Train Loss: 0.0011572931285328601\n",
      "Epoch [37/200], Train Loss: 0.0011122147691927271\n",
      "Epoch [38/200], Train Loss: 0.0010849765334654759\n",
      "Epoch [39/200], Train Loss: 0.0011369017303576438\n",
      "Epoch [40/200], Train Loss: 0.001145507401839564\n",
      "Epoch [41/200], Train Loss: 0.001021360141303174\n",
      "Epoch [42/200], Train Loss: 0.0010700532603044198\n",
      "Epoch [43/200], Train Loss: 0.0010562685007976813\n",
      "Epoch [44/200], Train Loss: 0.0010711863073145467\n",
      "Epoch [45/200], Train Loss: 0.0010723603748266537\n",
      "Epoch [46/200], Train Loss: 0.001064428169092994\n",
      "Epoch [47/200], Train Loss: 0.0010485474604102717\n",
      "Epoch [48/200], Train Loss: 0.0010143837084732052\n",
      "Epoch [49/200], Train Loss: 0.0010247074534202533\n",
      "Epoch [50/200], Train Loss: 0.0010245204547071353\n",
      "Epoch [51/200], Train Loss: 0.0010082818190351667\n",
      "Epoch [52/200], Train Loss: 0.0009778151421815599\n",
      "Epoch [53/200], Train Loss: 0.001023508909776457\n",
      "Epoch [54/200], Train Loss: 0.0009647627273314017\n",
      "Epoch [55/200], Train Loss: 0.0010074830666280655\n",
      "Epoch [56/200], Train Loss: 0.0009942407826947673\n",
      "Epoch [57/200], Train Loss: 0.0009231614318336002\n",
      "Epoch [58/200], Train Loss: 0.0009749373900508289\n",
      "Epoch [59/200], Train Loss: 0.00098595232349903\n",
      "Epoch [60/200], Train Loss: 0.0009715495790240162\n",
      "Epoch [61/200], Train Loss: 0.0009701093473253745\n",
      "Epoch [62/200], Train Loss: 0.000913216388717414\n",
      "Epoch [63/200], Train Loss: 0.0009541471209194915\n",
      "Epoch [64/200], Train Loss: 0.0009828128098096453\n",
      "Epoch [65/200], Train Loss: 0.0009246568378682443\n",
      "Epoch [66/200], Train Loss: 0.0009420133138558436\n",
      "Epoch [67/200], Train Loss: 0.000944732292392109\n",
      "Epoch [68/200], Train Loss: 0.0009451713339955943\n",
      "Epoch [69/200], Train Loss: 0.0008989459094078422\n",
      "Epoch [70/200], Train Loss: 0.0009551097784917486\n",
      "Epoch [71/200], Train Loss: 0.0008986136527559592\n",
      "Epoch [72/200], Train Loss: 0.0009070322444931709\n",
      "Epoch [73/200], Train Loss: 0.0009244010982601788\n",
      "Epoch [74/200], Train Loss: 0.0009456978975525041\n",
      "Epoch [75/200], Train Loss: 0.0008954003247896251\n",
      "Epoch [76/200], Train Loss: 0.0009787441420845743\n",
      "Epoch [77/200], Train Loss: 0.0008823088769976419\n",
      "Epoch [78/200], Train Loss: 0.0009204090129012773\n",
      "Epoch [79/200], Train Loss: 0.0009525166688806583\n",
      "Epoch [80/200], Train Loss: 0.0008846604217234221\n",
      "Epoch [81/200], Train Loss: 0.0008880740677689856\n",
      "Epoch [82/200], Train Loss: 0.0009015068070549576\n",
      "Epoch [83/200], Train Loss: 0.0009089813736287727\n",
      "Epoch [84/200], Train Loss: 0.0008954662689089173\n",
      "Epoch [85/200], Train Loss: 0.0008935908689206123\n",
      "Epoch [86/200], Train Loss: 0.0009267010480985701\n",
      "Epoch [87/200], Train Loss: 0.0009214699095615333\n",
      "Epoch [88/200], Train Loss: 0.0008888367083936935\n",
      "Epoch [89/200], Train Loss: 0.0008945467963743703\n",
      "Epoch [90/200], Train Loss: 0.0008990228524414283\n",
      "Epoch [91/200], Train Loss: 0.0009178267447136748\n",
      "Epoch [92/200], Train Loss: 0.0008989808839210643\n",
      "Epoch [93/200], Train Loss: 0.0008914636078365318\n",
      "Epoch [94/200], Train Loss: 0.000895640571540825\n",
      "Epoch [95/200], Train Loss: 0.0008774730059489149\n",
      "Epoch [96/200], Train Loss: 0.0008605242108404357\n",
      "Epoch [97/200], Train Loss: 0.0008676404513572276\n",
      "Epoch [98/200], Train Loss: 0.0008670703384145242\n",
      "Epoch [99/200], Train Loss: 0.0008661008486137185\n",
      "Epoch [100/200], Train Loss: 0.0009134443150241307\n",
      "Epoch [101/200], Train Loss: 0.0008713944346743855\n",
      "Epoch [102/200], Train Loss: 0.0008842957448842663\n",
      "Epoch [103/200], Train Loss: 0.0008905860520463687\n",
      "Epoch [104/200], Train Loss: 0.0008571893555232696\n",
      "Epoch [105/200], Train Loss: 0.0008452103127642854\n",
      "Epoch [106/200], Train Loss: 0.0009153882032125214\n",
      "Epoch [107/200], Train Loss: 0.0009031022026193252\n",
      "Epoch [108/200], Train Loss: 0.0008696020605150948\n",
      "Epoch [109/200], Train Loss: 0.000912635818848112\n",
      "Epoch [110/200], Train Loss: 0.0008705638731845608\n",
      "Epoch [111/200], Train Loss: 0.0008929051857675709\n",
      "Epoch [112/200], Train Loss: 0.0008668278578399879\n",
      "Epoch [113/200], Train Loss: 0.0008922672981049955\n",
      "Epoch [114/200], Train Loss: 0.0008652863524572783\n",
      "Epoch [115/200], Train Loss: 0.0008395566870874301\n",
      "Epoch [116/200], Train Loss: 0.0008891363220072001\n",
      "Epoch [117/200], Train Loss: 0.0008093065977716731\n",
      "Epoch [118/200], Train Loss: 0.0008665364967371341\n",
      "Epoch [119/200], Train Loss: 0.0008595845563552127\n",
      "Epoch [120/200], Train Loss: 0.0008119097843848477\n",
      "Epoch [121/200], Train Loss: 0.000842774683960824\n",
      "Epoch [122/200], Train Loss: 0.0008307353320887926\n",
      "Epoch [123/200], Train Loss: 0.0008678218017453983\n",
      "Epoch [124/200], Train Loss: 0.0008693003105210635\n",
      "Epoch [125/200], Train Loss: 0.0008774225126244335\n",
      "Epoch [126/200], Train Loss: 0.0008372070829458705\n",
      "Epoch [127/200], Train Loss: 0.0008890044180024598\n",
      "Epoch [128/200], Train Loss: 0.0008301975613753184\n",
      "Epoch [129/200], Train Loss: 0.0008324498342255415\n",
      "Epoch [130/200], Train Loss: 0.0008463767772896222\n",
      "Epoch [131/200], Train Loss: 0.0008572732757164088\n",
      "Epoch [132/200], Train Loss: 0.0008638608760402005\n",
      "Epoch [133/200], Train Loss: 0.0008362570849896592\n",
      "Epoch [134/200], Train Loss: 0.0008355619719299346\n",
      "Epoch [135/200], Train Loss: 0.0008531916043277942\n",
      "Epoch [136/200], Train Loss: 0.0008665294099164728\n",
      "Epoch [137/200], Train Loss: 0.0008658126377633553\n",
      "Epoch [138/200], Train Loss: 0.0008249569322575112\n",
      "Epoch [139/200], Train Loss: 0.0008049889701132321\n",
      "Epoch [140/200], Train Loss: 0.0008325344763037164\n",
      "Epoch [141/200], Train Loss: 0.0008348845010847971\n",
      "Epoch [142/200], Train Loss: 0.0008567034630326499\n",
      "Epoch [143/200], Train Loss: 0.0008559415307845508\n",
      "Epoch [144/200], Train Loss: 0.000848952391514381\n",
      "Epoch [145/200], Train Loss: 0.0008290852107270196\n",
      "Epoch [146/200], Train Loss: 0.000849236148093691\n",
      "Epoch [147/200], Train Loss: 0.0008679569790773021\n",
      "Epoch [148/200], Train Loss: 0.0008229644791295356\n",
      "Epoch [149/200], Train Loss: 0.0008048423989352648\n",
      "Epoch [150/200], Train Loss: 0.0008187201497526876\n",
      "Epoch [151/200], Train Loss: 0.0008287706755094396\n",
      "Epoch [152/200], Train Loss: 0.0008149346675723184\n",
      "Epoch [153/200], Train Loss: 0.0008446255964149613\n",
      "Epoch [154/200], Train Loss: 0.0008171910710794903\n",
      "Epoch [155/200], Train Loss: 0.0008019045468823829\n",
      "Epoch [156/200], Train Loss: 0.0008030161717657674\n",
      "Epoch [157/200], Train Loss: 0.0008468184512150455\n",
      "Epoch [158/200], Train Loss: 0.0008594255236152322\n",
      "Epoch [159/200], Train Loss: 0.0008436075151699742\n",
      "Epoch [160/200], Train Loss: 0.0008190050522905788\n",
      "Epoch [161/200], Train Loss: 0.0008241970834945786\n",
      "Epoch [162/200], Train Loss: 0.0008520154167704935\n",
      "Epoch [163/200], Train Loss: 0.000810583927988887\n",
      "Epoch [164/200], Train Loss: 0.000850487367697473\n",
      "Epoch [165/200], Train Loss: 0.0008466519913065454\n",
      "Epoch [166/200], Train Loss: 0.0008383656431250869\n",
      "Epoch [167/200], Train Loss: 0.0008194335387278084\n",
      "Epoch [168/200], Train Loss: 0.0007942092496298576\n",
      "Epoch [169/200], Train Loss: 0.000812531190893535\n",
      "Epoch [170/200], Train Loss: 0.0008068241375089275\n",
      "Epoch [171/200], Train Loss: 0.000788630209557349\n",
      "Epoch [172/200], Train Loss: 0.0008243438539028744\n",
      "Epoch [173/200], Train Loss: 0.0008153658269326575\n",
      "Epoch [174/200], Train Loss: 0.0008379114472860959\n",
      "Epoch [175/200], Train Loss: 0.0008813895758038333\n",
      "Epoch [176/200], Train Loss: 0.0008097817739055916\n",
      "Epoch [177/200], Train Loss: 0.0007864046397561577\n",
      "Epoch [178/200], Train Loss: 0.0008042081032052146\n",
      "Epoch [179/200], Train Loss: 0.00079720136844975\n",
      "Epoch [180/200], Train Loss: 0.0007959963148824065\n",
      "Epoch [181/200], Train Loss: 0.0008125510874407419\n",
      "Epoch [182/200], Train Loss: 0.0008043781486714764\n",
      "Epoch [183/200], Train Loss: 0.0007952902786087031\n",
      "Epoch [184/200], Train Loss: 0.0008147300363585384\n",
      "Epoch [185/200], Train Loss: 0.000805836939184612\n",
      "Epoch [186/200], Train Loss: 0.000778503183509506\n",
      "Epoch [187/200], Train Loss: 0.000795179220877079\n",
      "Epoch [188/200], Train Loss: 0.0008298705417812818\n",
      "Epoch [189/200], Train Loss: 0.0007989462458597182\n",
      "Epoch [190/200], Train Loss: 0.0008003833609061847\n",
      "Epoch [191/200], Train Loss: 0.0007952259439274406\n",
      "Epoch [192/200], Train Loss: 0.0008023825856075941\n",
      "Epoch [193/200], Train Loss: 0.0007744478046237776\n",
      "Epoch [194/200], Train Loss: 0.0008296694709487427\n",
      "Epoch [195/200], Train Loss: 0.0008238799822924421\n",
      "Epoch [196/200], Train Loss: 0.0008213749239108898\n",
      "Epoch [197/200], Train Loss: 0.0007951591747885477\n",
      "Epoch [198/200], Train Loss: 0.0007882883194996399\n",
      "Epoch [199/200], Train Loss: 0.0007851164316313191\n",
      "Epoch [200/200], Train Loss: 0.0007964151297980371\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    epoch_loss = 0\n",
    "    for batch in train_loader:\n",
    "        X, y = batch\n",
    "        X = X.clone().detach().float()\n",
    "        y = y.clone().detach().float()  \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X)\n",
    "        loss = loss_fn(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    # Calculate average loss for the epoch\n",
    "    average_loss = epoch_loss / len(train_loader)\n",
    "    train_losses.append(average_loss)\n",
    "\n",
    "    # Calculate training accuracy (R² score)\n",
    "    with torch.no_grad():\n",
    "        y_train_pred = model(torch.tensor(X_train.values, dtype=torch.float32)).flatten().numpy()\n",
    "        r2 = r2_score(y_train.values, y_train_pred)\n",
    "        accuracy = r2 * 100\n",
    "        train_accuracies.append(accuracy)\n",
    "\n",
    "    # Validate the model\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            X, y = batch\n",
    "            X = X.clone().detach().float()\n",
    "            y = y.clone().detach().float()  \n",
    "            outputs = model(X)\n",
    "            loss = loss_fn(outputs, y)\n",
    "            val_loss += loss.item()\n",
    "        \n",
    "        average_val_loss = val_loss / len(val_loader)\n",
    "        val_losses.append(average_val_loss)\n",
    "\n",
    "        y_val_pred = model(torch.tensor(X_val.values, dtype=torch.float32)).flatten().numpy()\n",
    "        val_r2 = r2_score(y_val.values, y_val_pred)\n",
    "        val_accuracy = val_r2 * 100\n",
    "        val_accuracies.append(val_accuracy)\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{n_epochs}], Train Loss: {average_loss}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained model and metrics\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'train_losses': train_losses,\n",
    "    'train_accuracies': train_accuracies,\n",
    "    'val_losses': val_losses,\n",
    "    'val_accuracies': val_accuracies\n",
    "}, 'models/qcnn_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error on test data: 0.00045295918787563715\n",
      "R² score (Accuracy) on test data: 0.9884499426723733\n",
      "Accuracy: 98.84499426723733%\n",
      "Mean Absolute Error (MAE) on test data: 0.01524676402903017\n",
      "Root Mean Squared Error (RMSE) on test data: 0.02128283787176036\n",
      "Mean Absolute Percentage Error (MAPE) on test data: 86.85117784328412%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "# Test the model on the test set\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_test_pred = model(X_test_tensor).flatten().numpy()\n",
    "    \n",
    "    # Calculate MSE, MAE, RMSE, R² score, and MAPE\n",
    "    test_mse = mean_squared_error(y_test.values, y_test_pred)\n",
    "    test_r2 = r2_score(y_test.values, y_test_pred)\n",
    "    test_mae = mean_absolute_error(y_test.values, y_test_pred)\n",
    "    test_rmse = mean_squared_error(y_test.values, y_test_pred, squared=False)\n",
    "    test_mape = np.mean(np.abs((y_test.values - y_test_pred) / y_test.values)) * 100\n",
    "\n",
    "    # Calculate accuracy as R² score in percentage\n",
    "    test_accuracy = test_r2 * 100\n",
    "    \n",
    "    print(f'Mean Squared Error on test data: {test_mse}')\n",
    "    print(f'R² score (Accuracy) on test data: {test_r2}')\n",
    "    print(f'Accuracy: {test_accuracy}%')\n",
    "    print(f'Mean Absolute Error (MAE) on test data: {test_mae}')\n",
    "    print(f'Root Mean Squared Error (RMSE) on test data: {test_rmse}')\n",
    "    print(f'Mean Absolute Percentage Error (MAPE) on test data: {test_mape}%')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
