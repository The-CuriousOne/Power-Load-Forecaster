{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset class\n",
    "class EnergyDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X = self.X.iloc[idx]\n",
    "        y = self.y.iloc[idx]\n",
    "        return X.values, y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "X_train = pd.read_csv('X_train.csv')\n",
    "y_train = pd.read_csv('y_train.csv')\n",
    "X_val = pd.read_csv('X_val.csv')\n",
    "y_val = pd.read_csv('y_val.csv')\n",
    "X_test = pd.read_csv('X_test.csv')\n",
    "y_test = pd.read_csv('y_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset\n",
    "train_dataset = EnergyDataset(X_train, y_train)\n",
    "val_dataset = EnergyDataset(X_val, y_val)\n",
    "test_dataset = EnergyDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loader\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the QCNN model with 5 hidden layers\n",
    "class QCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(QCNN, self).__init__()\n",
    "        \n",
    "        # Input layer (24 features to 256 neurons)\n",
    "        self.fc1 = nn.Linear(24, 256)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        \n",
    "        # Hidden layer 1 (256 to 128 neurons)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        \n",
    "        # Hidden layer 2 (128 to 64 neurons)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "        \n",
    "        # Hidden layer 3 (64 to 32 neurons)\n",
    "        self.fc4 = nn.Linear(64, 32)\n",
    "        self.bn4 = nn.BatchNorm1d(32)\n",
    "        \n",
    "        # Hidden layer 4 (32 to 16 neurons)\n",
    "        self.fc5 = nn.Linear(32, 16) \n",
    "        self.bn5 = nn.BatchNorm1d(16) \n",
    "        \n",
    "        # Hidden layer 5 (16 to 8 neurons)\n",
    "        self.fc6 = nn.Linear(16, 8) \n",
    "        self.bn6 = nn.BatchNorm1d(8)\n",
    "        \n",
    "        # Output layer (8 neurons to 1 output)\n",
    "        self.fc7 = nn.Linear(8, 1)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Input Layer\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Hidden Layer 1\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        # Hidden Layer 2\n",
    "        x = self.fc3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        # Hidden Layer 3\n",
    "        x = self.fc4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        # Hidden Layer 4\n",
    "        x = self.fc5(x) \n",
    "        x = self.bn5(x)  \n",
    "        x = self.relu(x)\n",
    "        \n",
    "        # Hidden Layer 5\n",
    "        x = self.fc6(x) \n",
    "        x = self.bn6(x)  \n",
    "        x = self.relu(x)  \n",
    "        \n",
    "        # Output Layer\n",
    "        x = self.fc7(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters\n",
    "learning_rate = 0.001\n",
    "n_epochs = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model, optimizer, and loss function\n",
    "model = QCNN()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists to log training and validation loss and accuracy\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/400], Train Loss: 0.010032180674262394\n",
      "Epoch [2/400], Train Loss: 0.0033365067067547584\n",
      "Epoch [3/400], Train Loss: 0.0029201389564659005\n",
      "Epoch [4/400], Train Loss: 0.0024905729663756834\n",
      "Epoch [5/400], Train Loss: 0.002262585412715762\n",
      "Epoch [6/400], Train Loss: 0.0021882108538660476\n",
      "Epoch [7/400], Train Loss: 0.0021109728771292226\n",
      "Epoch [8/400], Train Loss: 0.0019748213247810717\n",
      "Epoch [9/400], Train Loss: 0.0018949165316165038\n",
      "Epoch [10/400], Train Loss: 0.001941418939085211\n",
      "Epoch [11/400], Train Loss: 0.0018337017699370023\n",
      "Epoch [12/400], Train Loss: 0.001809046736243272\n",
      "Epoch [13/400], Train Loss: 0.001710603874910749\n",
      "Epoch [14/400], Train Loss: 0.001647221962596512\n",
      "Epoch [15/400], Train Loss: 0.0015471580926212937\n",
      "Epoch [16/400], Train Loss: 0.0015086839279488356\n",
      "Epoch [17/400], Train Loss: 0.0015175656251675501\n",
      "Epoch [18/400], Train Loss: 0.0015060277809777493\n",
      "Epoch [19/400], Train Loss: 0.0014328977580011046\n",
      "Epoch [20/400], Train Loss: 0.001409736529100113\n",
      "Epoch [21/400], Train Loss: 0.0013141432339768746\n",
      "Epoch [22/400], Train Loss: 0.0013218586715733611\n",
      "Epoch [23/400], Train Loss: 0.0013586178940227268\n",
      "Epoch [24/400], Train Loss: 0.0014588514250621803\n",
      "Epoch [25/400], Train Loss: 0.0012902263410415529\n",
      "Epoch [26/400], Train Loss: 0.001217223082530504\n",
      "Epoch [27/400], Train Loss: 0.001292053705385359\n",
      "Epoch [28/400], Train Loss: 0.0011917459875816659\n",
      "Epoch [29/400], Train Loss: 0.0012563537316250515\n",
      "Epoch [30/400], Train Loss: 0.0011847507130381052\n",
      "Epoch [31/400], Train Loss: 0.0011950760168804163\n",
      "Epoch [32/400], Train Loss: 0.0012102387147517558\n",
      "Epoch [33/400], Train Loss: 0.0012144477137480104\n",
      "Epoch [34/400], Train Loss: 0.0011663790039777042\n",
      "Epoch [35/400], Train Loss: 0.0012276171367350452\n",
      "Epoch [36/400], Train Loss: 0.0011315019719297797\n",
      "Epoch [37/400], Train Loss: 0.0011468145296550107\n",
      "Epoch [38/400], Train Loss: 0.0011386712159859162\n",
      "Epoch [39/400], Train Loss: 0.0010960233565347058\n",
      "Epoch [40/400], Train Loss: 0.0011324611810965505\n",
      "Epoch [41/400], Train Loss: 0.0011268961751794702\n",
      "Epoch [42/400], Train Loss: 0.0010573393160395624\n",
      "Epoch [43/400], Train Loss: 0.0010715611204259083\n",
      "Epoch [44/400], Train Loss: 0.0010820280144033915\n",
      "Epoch [45/400], Train Loss: 0.0010413939981813018\n",
      "Epoch [46/400], Train Loss: 0.0011013490093058106\n",
      "Epoch [47/400], Train Loss: 0.0010930929105339396\n",
      "Epoch [48/400], Train Loss: 0.001096594894654421\n",
      "Epoch [49/400], Train Loss: 0.0010190958637110354\n",
      "Epoch [50/400], Train Loss: 0.0011037222632503554\n",
      "Epoch [51/400], Train Loss: 0.0010324921345314245\n",
      "Epoch [52/400], Train Loss: 0.0010219616561026447\n",
      "Epoch [53/400], Train Loss: 0.0010337633300823677\n",
      "Epoch [54/400], Train Loss: 0.0010041235645214823\n",
      "Epoch [55/400], Train Loss: 0.0010271759770842175\n",
      "Epoch [56/400], Train Loss: 0.0010260992594185617\n",
      "Epoch [57/400], Train Loss: 0.0009972214491763473\n",
      "Epoch [58/400], Train Loss: 0.0009418471956801949\n",
      "Epoch [59/400], Train Loss: 0.000978376743562958\n",
      "Epoch [60/400], Train Loss: 0.0009842116457110847\n",
      "Epoch [61/400], Train Loss: 0.001009122578856278\n",
      "Epoch [62/400], Train Loss: 0.0009993823840942493\n",
      "Epoch [63/400], Train Loss: 0.0009880468722761001\n",
      "Epoch [64/400], Train Loss: 0.000978279596077299\n",
      "Epoch [65/400], Train Loss: 0.0009740869966151941\n",
      "Epoch [66/400], Train Loss: 0.0010113907327788678\n",
      "Epoch [67/400], Train Loss: 0.0009757979080716034\n",
      "Epoch [68/400], Train Loss: 0.000933415427971103\n",
      "Epoch [69/400], Train Loss: 0.000940905930296059\n",
      "Epoch [70/400], Train Loss: 0.0009386110350948296\n",
      "Epoch [71/400], Train Loss: 0.0009575461426317556\n",
      "Epoch [72/400], Train Loss: 0.0009384786205398431\n",
      "Epoch [73/400], Train Loss: 0.0009534610451398402\n",
      "Epoch [74/400], Train Loss: 0.0009557162397661012\n",
      "Epoch [75/400], Train Loss: 0.001017708807044257\n",
      "Epoch [76/400], Train Loss: 0.0009553809075754724\n",
      "Epoch [77/400], Train Loss: 0.0009439523369234755\n",
      "Epoch [78/400], Train Loss: 0.0009593840308371759\n",
      "Epoch [79/400], Train Loss: 0.0009680676219626385\n",
      "Epoch [80/400], Train Loss: 0.0009485005911511297\n",
      "Epoch [81/400], Train Loss: 0.0009704916182838046\n",
      "Epoch [82/400], Train Loss: 0.0008930421264614786\n",
      "Epoch [83/400], Train Loss: 0.0009117150916235285\n",
      "Epoch [84/400], Train Loss: 0.0009241528983655625\n",
      "Epoch [85/400], Train Loss: 0.0009248457086558492\n",
      "Epoch [86/400], Train Loss: 0.0009655590002591063\n",
      "Epoch [87/400], Train Loss: 0.0008866787999326538\n",
      "Epoch [88/400], Train Loss: 0.0009281964622913936\n",
      "Epoch [89/400], Train Loss: 0.0009248509923860703\n",
      "Epoch [90/400], Train Loss: 0.0009008523525965838\n",
      "Epoch [91/400], Train Loss: 0.0009423939877083753\n",
      "Epoch [92/400], Train Loss: 0.0009062823389381817\n",
      "Epoch [93/400], Train Loss: 0.0009038040218921864\n",
      "Epoch [94/400], Train Loss: 0.0008793952356700056\n",
      "Epoch [95/400], Train Loss: 0.0009049859782688458\n",
      "Epoch [96/400], Train Loss: 0.0009329464664156676\n",
      "Epoch [97/400], Train Loss: 0.0009378588526299335\n",
      "Epoch [98/400], Train Loss: 0.0009694400210752525\n",
      "Epoch [99/400], Train Loss: 0.0009008050078929571\n",
      "Epoch [100/400], Train Loss: 0.0008760879644472942\n",
      "Epoch [101/400], Train Loss: 0.0008926818468620104\n",
      "Epoch [102/400], Train Loss: 0.0008773288364820255\n",
      "Epoch [103/400], Train Loss: 0.0008942461909728994\n",
      "Epoch [104/400], Train Loss: 0.0009125912495950392\n",
      "Epoch [105/400], Train Loss: 0.0008963748657366509\n",
      "Epoch [106/400], Train Loss: 0.0009048152989499949\n",
      "Epoch [107/400], Train Loss: 0.0008964748299380733\n",
      "Epoch [108/400], Train Loss: 0.0008928469523609612\n",
      "Epoch [109/400], Train Loss: 0.0008559497528063799\n",
      "Epoch [110/400], Train Loss: 0.0008998553895197549\n",
      "Epoch [111/400], Train Loss: 0.0008895108186001762\n",
      "Epoch [112/400], Train Loss: 0.0008869360229253922\n",
      "Epoch [113/400], Train Loss: 0.0008627117487463512\n",
      "Epoch [114/400], Train Loss: 0.0008698579917265396\n",
      "Epoch [115/400], Train Loss: 0.0008951020346794489\n",
      "Epoch [116/400], Train Loss: 0.0008518241659763623\n",
      "Epoch [117/400], Train Loss: 0.0008547226776948109\n",
      "Epoch [118/400], Train Loss: 0.0008896164056085198\n",
      "Epoch [119/400], Train Loss: 0.000862530996219918\n",
      "Epoch [120/400], Train Loss: 0.0009175063872596339\n",
      "Epoch [121/400], Train Loss: 0.0008610262089620515\n",
      "Epoch [122/400], Train Loss: 0.0008748393330923094\n",
      "Epoch [123/400], Train Loss: 0.0008487479021960038\n",
      "Epoch [124/400], Train Loss: 0.000853867223892316\n",
      "Epoch [125/400], Train Loss: 0.0008753688376159511\n",
      "Epoch [126/400], Train Loss: 0.0008666583421241523\n",
      "Epoch [127/400], Train Loss: 0.0008986082525724713\n",
      "Epoch [128/400], Train Loss: 0.000845951264527732\n",
      "Epoch [129/400], Train Loss: 0.0008137878968349732\n",
      "Epoch [130/400], Train Loss: 0.0008799221488523311\n",
      "Epoch [131/400], Train Loss: 0.0008585651973875621\n",
      "Epoch [132/400], Train Loss: 0.0008861162754437781\n",
      "Epoch [133/400], Train Loss: 0.0008506208901901303\n",
      "Epoch [134/400], Train Loss: 0.0008454313654823817\n",
      "Epoch [135/400], Train Loss: 0.0008241434162588866\n",
      "Epoch [136/400], Train Loss: 0.000878148094074457\n",
      "Epoch [137/400], Train Loss: 0.0008678172507524889\n",
      "Epoch [138/400], Train Loss: 0.00084712434036571\n",
      "Epoch [139/400], Train Loss: 0.0008304610470987703\n",
      "Epoch [140/400], Train Loss: 0.0008576680767086409\n",
      "Epoch [141/400], Train Loss: 0.0008535669748704744\n",
      "Epoch [142/400], Train Loss: 0.0008532061352783989\n",
      "Epoch [143/400], Train Loss: 0.000836123043997274\n",
      "Epoch [144/400], Train Loss: 0.00082452190426956\n",
      "Epoch [145/400], Train Loss: 0.0008299428215616163\n",
      "Epoch [146/400], Train Loss: 0.0008852005441933555\n",
      "Epoch [147/400], Train Loss: 0.0008618704213626012\n",
      "Epoch [148/400], Train Loss: 0.000849224164050784\n",
      "Epoch [149/400], Train Loss: 0.0008496941555099564\n",
      "Epoch [150/400], Train Loss: 0.0007844466800934112\n",
      "Epoch [151/400], Train Loss: 0.0008053707290984778\n",
      "Epoch [152/400], Train Loss: 0.0008322468726875107\n",
      "Epoch [153/400], Train Loss: 0.0008262020594468025\n",
      "Epoch [154/400], Train Loss: 0.0008355708801684806\n",
      "Epoch [155/400], Train Loss: 0.0008603239463016298\n",
      "Epoch [156/400], Train Loss: 0.0008536007250117312\n",
      "Epoch [157/400], Train Loss: 0.000874122408741716\n",
      "Epoch [158/400], Train Loss: 0.00085206476629926\n",
      "Epoch [159/400], Train Loss: 0.0008407021188398768\n",
      "Epoch [160/400], Train Loss: 0.0008397608659396524\n",
      "Epoch [161/400], Train Loss: 0.0008958381354946017\n",
      "Epoch [162/400], Train Loss: 0.0008350649468447149\n",
      "Epoch [163/400], Train Loss: 0.0008392275742943019\n",
      "Epoch [164/400], Train Loss: 0.0008333601876041479\n",
      "Epoch [165/400], Train Loss: 0.0008372999027640361\n",
      "Epoch [166/400], Train Loss: 0.0008138673923541749\n",
      "Epoch [167/400], Train Loss: 0.0008030003782135897\n",
      "Epoch [168/400], Train Loss: 0.0008256639326382865\n",
      "Epoch [169/400], Train Loss: 0.0008332463724458544\n",
      "Epoch [170/400], Train Loss: 0.0008590466674619732\n",
      "Epoch [171/400], Train Loss: 0.0008100371062307834\n",
      "Epoch [172/400], Train Loss: 0.0008225986803699688\n",
      "Epoch [173/400], Train Loss: 0.0008061734661311178\n",
      "Epoch [174/400], Train Loss: 0.0008278904619931475\n",
      "Epoch [175/400], Train Loss: 0.0008293998854115212\n",
      "Epoch [176/400], Train Loss: 0.0008269522395340126\n",
      "Epoch [177/400], Train Loss: 0.0008445179309301938\n",
      "Epoch [178/400], Train Loss: 0.0008141350731462473\n",
      "Epoch [179/400], Train Loss: 0.000809008254215936\n",
      "Epoch [180/400], Train Loss: 0.0008119728519111347\n",
      "Epoch [181/400], Train Loss: 0.0008773023703806721\n",
      "Epoch [182/400], Train Loss: 0.0008065173141518668\n",
      "Epoch [183/400], Train Loss: 0.0008137955013205718\n",
      "Epoch [184/400], Train Loss: 0.0008022254589787834\n",
      "Epoch [185/400], Train Loss: 0.000834325899143407\n",
      "Epoch [186/400], Train Loss: 0.0008360533359217078\n",
      "Epoch [187/400], Train Loss: 0.0008101393958168758\n",
      "Epoch [188/400], Train Loss: 0.0008016701249194464\n",
      "Epoch [189/400], Train Loss: 0.0008585846639797456\n",
      "Epoch [190/400], Train Loss: 0.0008238021201735504\n",
      "Epoch [191/400], Train Loss: 0.0008320416593022591\n",
      "Epoch [192/400], Train Loss: 0.0007972660331198802\n",
      "Epoch [193/400], Train Loss: 0.0008108386753915932\n",
      "Epoch [194/400], Train Loss: 0.0008028444304037475\n",
      "Epoch [195/400], Train Loss: 0.0008349187761195176\n",
      "Epoch [196/400], Train Loss: 0.0008095142880181152\n",
      "Epoch [197/400], Train Loss: 0.0007829314214518935\n",
      "Epoch [198/400], Train Loss: 0.000799686880573549\n",
      "Epoch [199/400], Train Loss: 0.0008784586020447545\n",
      "Epoch [200/400], Train Loss: 0.0008018619178340805\n",
      "Epoch [201/400], Train Loss: 0.0008346124572415\n",
      "Epoch [202/400], Train Loss: 0.0007960959832834242\n",
      "Epoch [203/400], Train Loss: 0.0008044898217005748\n",
      "Epoch [204/400], Train Loss: 0.0007971670921438712\n",
      "Epoch [205/400], Train Loss: 0.0007805743914129725\n",
      "Epoch [206/400], Train Loss: 0.0008216683008619117\n",
      "Epoch [207/400], Train Loss: 0.0007833960674253246\n",
      "Epoch [208/400], Train Loss: 0.0008008504774798779\n",
      "Epoch [209/400], Train Loss: 0.0008352480071493287\n",
      "Epoch [210/400], Train Loss: 0.0008142231489262135\n",
      "Epoch [211/400], Train Loss: 0.0008220729120647914\n",
      "Epoch [212/400], Train Loss: 0.0008058686653764389\n",
      "Epoch [213/400], Train Loss: 0.0007776169666167133\n",
      "Epoch [214/400], Train Loss: 0.0007902618619110047\n",
      "Epoch [215/400], Train Loss: 0.000803351981494475\n",
      "Epoch [216/400], Train Loss: 0.0008508294402138115\n",
      "Epoch [217/400], Train Loss: 0.0008565678182370704\n",
      "Epoch [218/400], Train Loss: 0.0007961950016672744\n",
      "Epoch [219/400], Train Loss: 0.0008262781284358219\n",
      "Epoch [220/400], Train Loss: 0.0008203097442444293\n",
      "Epoch [221/400], Train Loss: 0.0007912371595765949\n",
      "Epoch [222/400], Train Loss: 0.0007954028385647024\n",
      "Epoch [223/400], Train Loss: 0.0008389099871682372\n",
      "Epoch [224/400], Train Loss: 0.0008068556648878213\n",
      "Epoch [225/400], Train Loss: 0.0008048689711119219\n",
      "Epoch [226/400], Train Loss: 0.0007818011645201261\n",
      "Epoch [227/400], Train Loss: 0.0008348707918336037\n",
      "Epoch [228/400], Train Loss: 0.0008119620366697108\n",
      "Epoch [229/400], Train Loss: 0.0008098938797545623\n",
      "Epoch [230/400], Train Loss: 0.0007952324278396767\n",
      "Epoch [231/400], Train Loss: 0.0008175754540956869\n",
      "Epoch [232/400], Train Loss: 0.0008072597750646549\n",
      "Epoch [233/400], Train Loss: 0.000840498037828889\n",
      "Epoch [234/400], Train Loss: 0.0008033771892440975\n",
      "Epoch [235/400], Train Loss: 0.0007785687924794078\n",
      "Epoch [236/400], Train Loss: 0.000788732986945648\n",
      "Epoch [237/400], Train Loss: 0.0008310563143075904\n",
      "Epoch [238/400], Train Loss: 0.0007925854230381857\n",
      "Epoch [239/400], Train Loss: 0.000808184531215588\n",
      "Epoch [240/400], Train Loss: 0.0008321788509265851\n",
      "Epoch [241/400], Train Loss: 0.0007864967972060515\n",
      "Epoch [242/400], Train Loss: 0.0007974183779931415\n",
      "Epoch [243/400], Train Loss: 0.000817444737049122\n",
      "Epoch [244/400], Train Loss: 0.0008012293413549484\n",
      "Epoch [245/400], Train Loss: 0.000815081929292735\n",
      "Epoch [246/400], Train Loss: 0.0007965593783802934\n",
      "Epoch [247/400], Train Loss: 0.0008371509739860766\n",
      "Epoch [248/400], Train Loss: 0.0007964890913585822\n",
      "Epoch [249/400], Train Loss: 0.0007768294608859149\n",
      "Epoch [250/400], Train Loss: 0.0007601233594701777\n",
      "Epoch [251/400], Train Loss: 0.0007822728545217219\n",
      "Epoch [252/400], Train Loss: 0.0007890458454012462\n",
      "Epoch [253/400], Train Loss: 0.0008211287104001334\n",
      "Epoch [254/400], Train Loss: 0.0008170233758898316\n",
      "Epoch [255/400], Train Loss: 0.0008041817372976869\n",
      "Epoch [256/400], Train Loss: 0.0008387624592824767\n",
      "Epoch [257/400], Train Loss: 0.000799732907674296\n",
      "Epoch [258/400], Train Loss: 0.0008281614375133747\n",
      "Epoch [259/400], Train Loss: 0.0008238534137964518\n",
      "Epoch [260/400], Train Loss: 0.0008049617986518992\n",
      "Epoch [261/400], Train Loss: 0.0007961737184193061\n",
      "Epoch [262/400], Train Loss: 0.0007887840167747693\n",
      "Epoch [263/400], Train Loss: 0.0007800489040337517\n",
      "Epoch [264/400], Train Loss: 0.000775466501280074\n",
      "Epoch [265/400], Train Loss: 0.0007847870726888451\n",
      "Epoch [266/400], Train Loss: 0.0007547276966570363\n",
      "Epoch [267/400], Train Loss: 0.0008002943337876459\n",
      "Epoch [268/400], Train Loss: 0.0008277907248547257\n",
      "Epoch [269/400], Train Loss: 0.0008203517921268029\n",
      "Epoch [270/400], Train Loss: 0.0007746856678395642\n",
      "Epoch [271/400], Train Loss: 0.0007939438737321443\n",
      "Epoch [272/400], Train Loss: 0.0008410266001966528\n",
      "Epoch [273/400], Train Loss: 0.0008326382274750676\n",
      "Epoch [274/400], Train Loss: 0.0007875219945273103\n",
      "Epoch [275/400], Train Loss: 0.0008159687877599286\n",
      "Epoch [276/400], Train Loss: 0.0007978501976347763\n",
      "Epoch [277/400], Train Loss: 0.000810281712162816\n",
      "Epoch [278/400], Train Loss: 0.0008063946485642654\n",
      "Epoch [279/400], Train Loss: 0.0007827549427679426\n",
      "Epoch [280/400], Train Loss: 0.00078953262075865\n",
      "Epoch [281/400], Train Loss: 0.000784908759275954\n",
      "Epoch [282/400], Train Loss: 0.000813163109007595\n",
      "Epoch [283/400], Train Loss: 0.000775468304104899\n",
      "Epoch [284/400], Train Loss: 0.0007969805213367455\n",
      "Epoch [285/400], Train Loss: 0.0008049193186706085\n",
      "Epoch [286/400], Train Loss: 0.0007992244824061943\n",
      "Epoch [287/400], Train Loss: 0.0007654519054731791\n",
      "Epoch [288/400], Train Loss: 0.0008389964152615933\n",
      "Epoch [289/400], Train Loss: 0.0008350116615635957\n",
      "Epoch [290/400], Train Loss: 0.0008192043942341647\n",
      "Epoch [291/400], Train Loss: 0.0008105763184186557\n",
      "Epoch [292/400], Train Loss: 0.0008341171698484782\n",
      "Epoch [293/400], Train Loss: 0.0007711480372564714\n",
      "Epoch [294/400], Train Loss: 0.0008082562061902463\n",
      "Epoch [295/400], Train Loss: 0.0007641059818343385\n",
      "Epoch [296/400], Train Loss: 0.0007989806443295895\n",
      "Epoch [297/400], Train Loss: 0.0008152194205309402\n",
      "Epoch [298/400], Train Loss: 0.0007816061861779677\n",
      "Epoch [299/400], Train Loss: 0.0007573741858046759\n",
      "Epoch [300/400], Train Loss: 0.000810073909997537\n",
      "Epoch [301/400], Train Loss: 0.0007945001596802568\n",
      "Epoch [302/400], Train Loss: 0.0007689139046302524\n",
      "Epoch [303/400], Train Loss: 0.0008431720504348411\n",
      "Epoch [304/400], Train Loss: 0.0007991754875305586\n",
      "Epoch [305/400], Train Loss: 0.0007968446861330549\n",
      "Epoch [306/400], Train Loss: 0.0007469687458743053\n",
      "Epoch [307/400], Train Loss: 0.0007934878901325013\n",
      "Epoch [308/400], Train Loss: 0.000815325101168621\n",
      "Epoch [309/400], Train Loss: 0.0008121994760396223\n",
      "Epoch [310/400], Train Loss: 0.0008076375720016488\n",
      "Epoch [311/400], Train Loss: 0.0007619768366282271\n",
      "Epoch [312/400], Train Loss: 0.0007648213799603313\n",
      "Epoch [313/400], Train Loss: 0.0007834689190750578\n",
      "Epoch [314/400], Train Loss: 0.0007898458548699478\n",
      "Epoch [315/400], Train Loss: 0.000789244968477956\n",
      "Epoch [316/400], Train Loss: 0.0007791436362744162\n",
      "Epoch [317/400], Train Loss: 0.0007868760124330362\n",
      "Epoch [318/400], Train Loss: 0.0008390391449590615\n",
      "Epoch [319/400], Train Loss: 0.0007687296898484004\n",
      "Epoch [320/400], Train Loss: 0.0008021931479972784\n",
      "Epoch [321/400], Train Loss: 0.0008072705975915226\n",
      "Epoch [322/400], Train Loss: 0.0007817451231024681\n",
      "Epoch [323/400], Train Loss: 0.0007927767543137047\n",
      "Epoch [324/400], Train Loss: 0.0007658554332224121\n",
      "Epoch [325/400], Train Loss: 0.0008425944576749026\n",
      "Epoch [326/400], Train Loss: 0.0008211346542408976\n",
      "Epoch [327/400], Train Loss: 0.0008099838893455629\n",
      "Epoch [328/400], Train Loss: 0.0007618159657745529\n",
      "Epoch [329/400], Train Loss: 0.0008020315034825882\n",
      "Epoch [330/400], Train Loss: 0.0007687725488055799\n",
      "Epoch [331/400], Train Loss: 0.0008342146623871779\n",
      "Epoch [332/400], Train Loss: 0.0007720977314058788\n",
      "Epoch [333/400], Train Loss: 0.0007581651391251428\n",
      "Epoch [334/400], Train Loss: 0.0008403906632116735\n",
      "Epoch [335/400], Train Loss: 0.0007760329433835829\n",
      "Epoch [336/400], Train Loss: 0.000767364574111652\n",
      "Epoch [337/400], Train Loss: 0.000791584056403823\n",
      "Epoch [338/400], Train Loss: 0.0007894634230246323\n",
      "Epoch [339/400], Train Loss: 0.0007785131265277242\n",
      "Epoch [340/400], Train Loss: 0.0007780717171170364\n",
      "Epoch [341/400], Train Loss: 0.0008083182952852162\n",
      "Epoch [342/400], Train Loss: 0.0007858065340511326\n",
      "Epoch [343/400], Train Loss: 0.0007623281222334037\n",
      "Epoch [344/400], Train Loss: 0.0007724341586328717\n",
      "Epoch [345/400], Train Loss: 0.000789341012825989\n",
      "Epoch [346/400], Train Loss: 0.0007988648090922581\n",
      "Epoch [347/400], Train Loss: 0.000757257120864104\n",
      "Epoch [348/400], Train Loss: 0.0007561342894502477\n",
      "Epoch [349/400], Train Loss: 0.0007818768534328511\n",
      "Epoch [350/400], Train Loss: 0.0008046120809159828\n",
      "Epoch [351/400], Train Loss: 0.0008125527848163559\n",
      "Epoch [352/400], Train Loss: 0.0007750589927851807\n",
      "Epoch [353/400], Train Loss: 0.0007758228388852336\n",
      "Epoch [354/400], Train Loss: 0.0007533975677846141\n",
      "Epoch [355/400], Train Loss: 0.0008006080952019162\n",
      "Epoch [356/400], Train Loss: 0.0007948611596849052\n",
      "Epoch [357/400], Train Loss: 0.0008432956040307766\n",
      "Epoch [358/400], Train Loss: 0.0007727789116423514\n",
      "Epoch [359/400], Train Loss: 0.0007924251429125851\n",
      "Epoch [360/400], Train Loss: 0.0008039019703147131\n",
      "Epoch [361/400], Train Loss: 0.00076538120335192\n",
      "Epoch [362/400], Train Loss: 0.0007455494239118261\n",
      "Epoch [363/400], Train Loss: 0.0007476641961703965\n",
      "Epoch [364/400], Train Loss: 0.0007661838142932581\n",
      "Epoch [365/400], Train Loss: 0.0007898986185777925\n",
      "Epoch [366/400], Train Loss: 0.0007454449613662097\n",
      "Epoch [367/400], Train Loss: 0.0008155207487091548\n",
      "Epoch [368/400], Train Loss: 0.0007516500116649206\n",
      "Epoch [369/400], Train Loss: 0.0008122934543002807\n",
      "Epoch [370/400], Train Loss: 0.0007490666201908865\n",
      "Epoch [371/400], Train Loss: 0.0007647129942980538\n",
      "Epoch [372/400], Train Loss: 0.0008030082574970243\n",
      "Epoch [373/400], Train Loss: 0.0007391022389833275\n",
      "Epoch [374/400], Train Loss: 0.0007946039788111391\n",
      "Epoch [375/400], Train Loss: 0.000782784411401823\n",
      "Epoch [376/400], Train Loss: 0.0007775878986063555\n",
      "Epoch [377/400], Train Loss: 0.0007599738293585114\n",
      "Epoch [378/400], Train Loss: 0.0007842425105187464\n",
      "Epoch [379/400], Train Loss: 0.000773239742945989\n",
      "Epoch [380/400], Train Loss: 0.0007732364661468562\n",
      "Epoch [381/400], Train Loss: 0.0007605758935829012\n",
      "Epoch [382/400], Train Loss: 0.000809041869974904\n",
      "Epoch [383/400], Train Loss: 0.0007856813565601146\n",
      "Epoch [384/400], Train Loss: 0.0008059447198135431\n",
      "Epoch [385/400], Train Loss: 0.0007911346645182183\n",
      "Epoch [386/400], Train Loss: 0.0007542997925810035\n",
      "Epoch [387/400], Train Loss: 0.0008018178360796542\n",
      "Epoch [388/400], Train Loss: 0.0007788219758453008\n",
      "Epoch [389/400], Train Loss: 0.0007761707758613543\n",
      "Epoch [390/400], Train Loss: 0.0007879075750883431\n",
      "Epoch [391/400], Train Loss: 0.0007498543781232613\n",
      "Epoch [392/400], Train Loss: 0.0007735400308235312\n",
      "Epoch [393/400], Train Loss: 0.0007811772248777834\n",
      "Epoch [394/400], Train Loss: 0.0007577179072787827\n",
      "Epoch [395/400], Train Loss: 0.0007487919406770034\n",
      "Epoch [396/400], Train Loss: 0.000750841495928244\n",
      "Epoch [397/400], Train Loss: 0.0007784656379745261\n",
      "Epoch [398/400], Train Loss: 0.0007883105963189883\n",
      "Epoch [399/400], Train Loss: 0.0007965613991613141\n",
      "Epoch [400/400], Train Loss: 0.0007976098422090385\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()  \n",
    "    epoch_loss = 0\n",
    "    for batch in train_loader:\n",
    "        X, y = batch\n",
    "        X = X.clone().detach().float()\n",
    "        y = y.clone().detach().float()  \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X)\n",
    "        loss = loss_fn(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    # Calculate average loss for the epoch\n",
    "    average_loss = epoch_loss / len(train_loader)\n",
    "    train_losses.append(average_loss)\n",
    "\n",
    "    # Calculate training accuracy (R² score)\n",
    "    with torch.no_grad():\n",
    "        y_train_pred = model(torch.tensor(X_train.values, dtype=torch.float32)).flatten().numpy()\n",
    "        r2 = r2_score(y_train.values, y_train_pred)\n",
    "        accuracy = r2 * 100\n",
    "        train_accuracies.append(accuracy)\n",
    "\n",
    "    # Validate the model\n",
    "    model.eval()  \n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            X, y = batch\n",
    "            X = X.clone().detach().float()\n",
    "            y = y.clone().detach().float()  \n",
    "            outputs = model(X)\n",
    "            loss = loss_fn(outputs, y)\n",
    "            val_loss += loss.item()\n",
    "        \n",
    "        average_val_loss = val_loss / len(val_loader)\n",
    "        val_losses.append(average_val_loss)\n",
    "\n",
    "        y_val_pred = model(torch.tensor(X_val.values, dtype=torch.float32)).flatten().numpy()\n",
    "        val_r2 = r2_score(y_val.values, y_val_pred)\n",
    "        val_accuracy = val_r2 * 100\n",
    "        val_accuracies.append(val_accuracy)\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{n_epochs}], Train Loss: {average_loss}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained model and metrics\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'train_losses': train_losses,\n",
    "    'train_accuracies': train_accuracies,\n",
    "    'val_losses': val_losses,\n",
    "    'val_accuracies': val_accuracies\n",
    "}, 'qcnn_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error on test data: 0.0005326356919208483\n",
      "R² score (Accuracy) on test data: 0.9864182625254199\n",
      "Accuracy: 98.64182625254199%\n"
     ]
    }
   ],
   "source": [
    "# Test the model on the test set\n",
    "model.eval()  \n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_test_pred = model(X_test_tensor).flatten().numpy()\n",
    "    \n",
    "    # Calculate MSE and R² score\n",
    "    test_mse = mean_squared_error(y_test.values, y_test_pred)\n",
    "    test_r2 = r2_score(y_test.values, y_test_pred)\n",
    "\n",
    "    # Calculate accuracy as R² score in percentage\n",
    "    test_accuracy = test_r2 * 100\n",
    "    \n",
    "    print(f'Mean Squared Error on test data: {test_mse}')\n",
    "    print(f'R² score (Accuracy) on test data: {test_r2}')\n",
    "    print(f'Accuracy: {test_accuracy}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
