{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training, validation, and test data\n",
    "X_train = pd.read_csv('X_train.csv')\n",
    "y_train = pd.read_csv('y_train.csv')\n",
    "X_val = pd.read_csv('X_val.csv')\n",
    "y_val = pd.read_csv('y_val.csv')\n",
    "X_test = pd.read_csv('X_test.csv')\n",
    "y_test = pd.read_csv('y_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to numpy arrays\n",
    "X_train_np = X_train.values\n",
    "y_train_np = y_train.values.flatten()\n",
    "X_val_np = X_val.values\n",
    "y_val_np = y_val.values.flatten()\n",
    "X_test_np = X_test.values\n",
    "y_test_np = y_test.values.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection using PCA to reduce features\n",
    "n_components = 10 \n",
    "pca = PCA(n_components=n_components)\n",
    "X_train_reduced = pca.fit_transform(X_train_np)\n",
    "X_val_reduced = pca.transform(X_val_np)\n",
    "X_test_reduced = pca.transform(X_test_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape data to fit CNN input format (batch_size, channels, height, width)\n",
    "X_train_reduced = X_train_reduced.reshape((X_train_reduced.shape[0], 1, 1, n_components))\n",
    "X_val_reduced = X_val_reduced.reshape((X_val_reduced.shape[0], 1, 1, n_components))\n",
    "X_test_reduced = X_test_reduced.reshape((X_test_reduced.shape[0], 1, 1, n_components))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define CNN model\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=(1, 3))\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=(1, 3))\n",
    "        self.fc1 = nn.Linear(32 * 1 * (n_components - 4), 128)\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.conv2(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = x.view(x.size(0), -1) \n",
    "        x = self.fc1(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model, loss function, and optimizer\n",
    "model = CNN()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to PyTorch tensors with float32 dtype\n",
    "X_train_tensor = torch.tensor(X_train_reduced, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train_np, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val_reduced, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val_np, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_reduced, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test_np, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader for batch processing\n",
    "batch_size = 1000 \n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists to log training loss and accuracy\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/400], Loss: 0.18143770337104798\n",
      "Epoch [2/400], Loss: 0.0849282719194889\n",
      "Epoch [3/400], Loss: 0.042863625884056095\n",
      "Epoch [4/400], Loss: 0.03701202601194382\n",
      "Epoch [5/400], Loss: 0.033344348073005674\n",
      "Epoch [6/400], Loss: 0.028579869642853736\n",
      "Epoch [7/400], Loss: 0.02354653097689152\n",
      "Epoch [8/400], Loss: 0.018677205964922904\n",
      "Epoch [9/400], Loss: 0.014252216964960098\n",
      "Epoch [10/400], Loss: 0.010274343565106392\n",
      "Epoch [11/400], Loss: 0.007034100219607353\n",
      "Epoch [12/400], Loss: 0.0047370801120996476\n",
      "Epoch [13/400], Loss: 0.00338000594638288\n",
      "Epoch [14/400], Loss: 0.0026107383891940117\n",
      "Epoch [15/400], Loss: 0.0021984754782170056\n",
      "Epoch [16/400], Loss: 0.001948431204073131\n",
      "Epoch [17/400], Loss: 0.0018111222283914686\n",
      "Epoch [18/400], Loss: 0.0016979382606223225\n",
      "Epoch [19/400], Loss: 0.0016123669454827904\n",
      "Epoch [20/400], Loss: 0.0015365476114675403\n",
      "Epoch [21/400], Loss: 0.001483995197340846\n",
      "Epoch [22/400], Loss: 0.001440093843266368\n",
      "Epoch [23/400], Loss: 0.001404731017537415\n",
      "Epoch [24/400], Loss: 0.001371692200191319\n",
      "Epoch [25/400], Loss: 0.0013408249989151954\n",
      "Epoch [26/400], Loss: 0.0013208511658012866\n",
      "Epoch [27/400], Loss: 0.0013012404181063175\n",
      "Epoch [28/400], Loss: 0.0012844385113567115\n",
      "Epoch [29/400], Loss: 0.0012736214650794864\n",
      "Epoch [30/400], Loss: 0.001261083078570664\n",
      "Epoch [31/400], Loss: 0.0012427973840385676\n",
      "Epoch [32/400], Loss: 0.001233689091168344\n",
      "Epoch [33/400], Loss: 0.0012261634971946478\n",
      "Epoch [34/400], Loss: 0.001218781783245504\n",
      "Epoch [35/400], Loss: 0.0012051014788448812\n",
      "Epoch [36/400], Loss: 0.0011954803625121712\n",
      "Epoch [37/400], Loss: 0.0011885777348652482\n",
      "Epoch [38/400], Loss: 0.001179810487665236\n",
      "Epoch [39/400], Loss: 0.0011720527103170752\n",
      "Epoch [40/400], Loss: 0.001169424019753933\n",
      "Epoch [41/400], Loss: 0.001162093710154295\n",
      "Epoch [42/400], Loss: 0.0011603403789922596\n",
      "Epoch [43/400], Loss: 0.0011512437416240572\n",
      "Epoch [44/400], Loss: 0.0011437851097434758\n",
      "Epoch [45/400], Loss: 0.0011398267280310391\n",
      "Epoch [46/400], Loss: 0.001131400540471077\n",
      "Epoch [47/400], Loss: 0.0011295800795778632\n",
      "Epoch [48/400], Loss: 0.001123521705158055\n",
      "Epoch [49/400], Loss: 0.0011151614342816173\n",
      "Epoch [50/400], Loss: 0.0011146380426362158\n",
      "Epoch [51/400], Loss: 0.0011091960687190294\n",
      "Epoch [52/400], Loss: 0.0011132224323228003\n",
      "Epoch [53/400], Loss: 0.0011118618166074156\n",
      "Epoch [54/400], Loss: 0.0011063707200810312\n",
      "Epoch [55/400], Loss: 0.0010973100853152572\n",
      "Epoch [56/400], Loss: 0.0010923526529222727\n",
      "Epoch [57/400], Loss: 0.0010861782636493445\n",
      "Epoch [58/400], Loss: 0.001082010215613991\n",
      "Epoch [59/400], Loss: 0.0010766473738476634\n",
      "Epoch [60/400], Loss: 0.0010728569282218814\n",
      "Epoch [61/400], Loss: 0.0010768358316272496\n",
      "Epoch [62/400], Loss: 0.0010676687280647456\n",
      "Epoch [63/400], Loss: 0.001070624371059239\n",
      "Epoch [64/400], Loss: 0.0010618683812208473\n",
      "Epoch [65/400], Loss: 0.0010592361935414373\n",
      "Epoch [66/400], Loss: 0.0010516614187508822\n",
      "Epoch [67/400], Loss: 0.0010492426692508162\n",
      "Epoch [68/400], Loss: 0.0010470150201581417\n",
      "Epoch [69/400], Loss: 0.0010505720321089028\n",
      "Epoch [70/400], Loss: 0.0010407323762774466\n",
      "Epoch [71/400], Loss: 0.0010396998142823577\n",
      "Epoch [72/400], Loss: 0.0010357417166233062\n",
      "Epoch [73/400], Loss: 0.001033905476797372\n",
      "Epoch [74/400], Loss: 0.0010318491118960083\n",
      "Epoch [75/400], Loss: 0.0010282170469872653\n",
      "Epoch [76/400], Loss: 0.001028625569306314\n",
      "Epoch [77/400], Loss: 0.001025993621442467\n",
      "Epoch [78/400], Loss: 0.001023335491772741\n",
      "Epoch [79/400], Loss: 0.001018411482218653\n",
      "Epoch [80/400], Loss: 0.0010226369183510542\n",
      "Epoch [81/400], Loss: 0.00101230395026505\n",
      "Epoch [82/400], Loss: 0.0010159674426540733\n",
      "Epoch [83/400], Loss: 0.0010148672154173255\n",
      "Epoch [84/400], Loss: 0.0010100498981773854\n",
      "Epoch [85/400], Loss: 0.0010056876041926444\n",
      "Epoch [86/400], Loss: 0.0010092333960346877\n",
      "Epoch [87/400], Loss: 0.0010002913069911302\n",
      "Epoch [88/400], Loss: 0.0009993528109043836\n",
      "Epoch [89/400], Loss: 0.0009959013620391489\n",
      "Epoch [90/400], Loss: 0.0009934799862094223\n",
      "Epoch [91/400], Loss: 0.0009995420440100133\n",
      "Epoch [92/400], Loss: 0.0009926955564878881\n",
      "Epoch [93/400], Loss: 0.0009897552384063601\n",
      "Epoch [94/400], Loss: 0.0009859350579790771\n",
      "Epoch [95/400], Loss: 0.0009898662311024964\n",
      "Epoch [96/400], Loss: 0.000983564374037087\n",
      "Epoch [97/400], Loss: 0.0009843186661601068\n",
      "Epoch [98/400], Loss: 0.0009802820114418864\n",
      "Epoch [99/400], Loss: 0.0009792577871121466\n",
      "Epoch [100/400], Loss: 0.000978868086822331\n",
      "Epoch [101/400], Loss: 0.0009749304363504052\n",
      "Epoch [102/400], Loss: 0.0009734322642907501\n",
      "Epoch [103/400], Loss: 0.000978346560150385\n",
      "Epoch [104/400], Loss: 0.0009766384889371693\n",
      "Epoch [105/400], Loss: 0.0009707025112584233\n",
      "Epoch [106/400], Loss: 0.0009733497654087842\n",
      "Epoch [107/400], Loss: 0.0009647148405201733\n",
      "Epoch [108/400], Loss: 0.0009678671439178288\n",
      "Epoch [109/400], Loss: 0.0009625765099190175\n",
      "Epoch [110/400], Loss: 0.0009626904805190861\n",
      "Epoch [111/400], Loss: 0.0009641421819105745\n",
      "Epoch [112/400], Loss: 0.0009618348232470452\n",
      "Epoch [113/400], Loss: 0.0009600381483323873\n",
      "Epoch [114/400], Loss: 0.0009553465782664716\n",
      "Epoch [115/400], Loss: 0.0009561305586248636\n",
      "Epoch [116/400], Loss: 0.0009550390113145113\n",
      "Epoch [117/400], Loss: 0.000959883569739759\n",
      "Epoch [118/400], Loss: 0.000950442310422659\n",
      "Epoch [119/400], Loss: 0.0009493913943879307\n",
      "Epoch [120/400], Loss: 0.0009506361815147102\n",
      "Epoch [121/400], Loss: 0.000945108796004206\n",
      "Epoch [122/400], Loss: 0.0009474961529485882\n",
      "Epoch [123/400], Loss: 0.0009489087434485555\n",
      "Epoch [124/400], Loss: 0.0009498766739852726\n",
      "Epoch [125/400], Loss: 0.0009499218454584479\n",
      "Epoch [126/400], Loss: 0.0009424049430526793\n",
      "Epoch [127/400], Loss: 0.0009389793407171965\n",
      "Epoch [128/400], Loss: 0.0009408508916385472\n",
      "Epoch [129/400], Loss: 0.0009441166138276458\n",
      "Epoch [130/400], Loss: 0.0009404784138314425\n",
      "Epoch [131/400], Loss: 0.0009402451058849692\n",
      "Epoch [132/400], Loss: 0.0009407484601251781\n",
      "Epoch [133/400], Loss: 0.0009404725045897066\n",
      "Epoch [134/400], Loss: 0.0009341947105713189\n",
      "Epoch [135/400], Loss: 0.0009360303357243538\n",
      "Epoch [136/400], Loss: 0.0009395484835840761\n",
      "Epoch [137/400], Loss: 0.0009302295208908618\n",
      "Epoch [138/400], Loss: 0.0009277677186764776\n",
      "Epoch [139/400], Loss: 0.000930668655782938\n",
      "Epoch [140/400], Loss: 0.0009302719170227647\n",
      "Epoch [141/400], Loss: 0.0009352714009582997\n",
      "Epoch [142/400], Loss: 0.0009263381408527494\n",
      "Epoch [143/400], Loss: 0.0009269696683622897\n",
      "Epoch [144/400], Loss: 0.0009247408621013164\n",
      "Epoch [145/400], Loss: 0.0009246431034989655\n",
      "Epoch [146/400], Loss: 0.0009221649053506553\n",
      "Epoch [147/400], Loss: 0.0009248575707897544\n",
      "Epoch [148/400], Loss: 0.0009199647326022386\n",
      "Epoch [149/400], Loss: 0.0009210569504648447\n",
      "Epoch [150/400], Loss: 0.0009188418206758797\n",
      "Epoch [151/400], Loss: 0.0009181646769866347\n",
      "Epoch [152/400], Loss: 0.0009187869960442186\n",
      "Epoch [153/400], Loss: 0.000918689426034689\n",
      "Epoch [154/400], Loss: 0.0009206997603178024\n",
      "Epoch [155/400], Loss: 0.000916840024292469\n",
      "Epoch [156/400], Loss: 0.0009159840410575271\n",
      "Epoch [157/400], Loss: 0.0009218047326430679\n",
      "Epoch [158/400], Loss: 0.0009264910803176463\n",
      "Epoch [159/400], Loss: 0.0009154545678757131\n",
      "Epoch [160/400], Loss: 0.0009131357539445162\n",
      "Epoch [161/400], Loss: 0.0009137583430856467\n",
      "Epoch [162/400], Loss: 0.0009136712108738721\n",
      "Epoch [163/400], Loss: 0.0009132213564589619\n",
      "Epoch [164/400], Loss: 0.0009081224701367319\n",
      "Epoch [165/400], Loss: 0.0009085493255406618\n",
      "Epoch [166/400], Loss: 0.0009106478071771562\n",
      "Epoch [167/400], Loss: 0.0009093975531868637\n",
      "Epoch [168/400], Loss: 0.0009066487662494182\n",
      "Epoch [169/400], Loss: 0.0009059581370092928\n",
      "Epoch [170/400], Loss: 0.0009066863777115941\n",
      "Epoch [171/400], Loss: 0.0009037417848594487\n",
      "Epoch [172/400], Loss: 0.0009043074864894152\n",
      "Epoch [173/400], Loss: 0.0009043208416551351\n",
      "Epoch [174/400], Loss: 0.0009037390467710793\n",
      "Epoch [175/400], Loss: 0.0009032684797421098\n",
      "Epoch [176/400], Loss: 0.0009029169799759984\n",
      "Epoch [177/400], Loss: 0.0009034085576422512\n",
      "Epoch [178/400], Loss: 0.000905564611312002\n",
      "Epoch [179/400], Loss: 0.0009055262012407183\n",
      "Epoch [180/400], Loss: 0.000899553271010518\n",
      "Epoch [181/400], Loss: 0.0009016399527899921\n",
      "Epoch [182/400], Loss: 0.0008988428092561663\n",
      "Epoch [183/400], Loss: 0.0008951609954237938\n",
      "Epoch [184/400], Loss: 0.0008978488389402628\n",
      "Epoch [185/400], Loss: 0.0009007662325166165\n",
      "Epoch [186/400], Loss: 0.0008954851492308081\n",
      "Epoch [187/400], Loss: 0.0009021650836803019\n",
      "Epoch [188/400], Loss: 0.0008952714432962238\n",
      "Epoch [189/400], Loss: 0.0008930033538490534\n",
      "Epoch [190/400], Loss: 0.0008927764440886676\n",
      "Epoch [191/400], Loss: 0.0008928684401325881\n",
      "Epoch [192/400], Loss: 0.000893299961462617\n",
      "Epoch [193/400], Loss: 0.000898518213070929\n",
      "Epoch [194/400], Loss: 0.0008914887555874884\n",
      "Epoch [195/400], Loss: 0.0008922836533747613\n",
      "Epoch [196/400], Loss: 0.0008913449686951936\n",
      "Epoch [197/400], Loss: 0.0008918382250703871\n",
      "Epoch [198/400], Loss: 0.0008924900810234249\n",
      "Epoch [199/400], Loss: 0.0008898462355136871\n",
      "Epoch [200/400], Loss: 0.000892124071251601\n",
      "Epoch [201/400], Loss: 0.0008856986952014268\n",
      "Epoch [202/400], Loss: 0.0008908645133487881\n",
      "Epoch [203/400], Loss: 0.0008871464664116502\n",
      "Epoch [204/400], Loss: 0.0008870904357172549\n",
      "Epoch [205/400], Loss: 0.0008827077643945813\n",
      "Epoch [206/400], Loss: 0.0008829861879348754\n",
      "Epoch [207/400], Loss: 0.0008785078302025795\n",
      "Epoch [208/400], Loss: 0.0008815177157521247\n",
      "Epoch [209/400], Loss: 0.0008817072329111397\n",
      "Epoch [210/400], Loss: 0.0008809336298145354\n",
      "Epoch [211/400], Loss: 0.0008856782387010753\n",
      "Epoch [212/400], Loss: 0.0008824076596647501\n",
      "Epoch [213/400], Loss: 0.0008790460554882884\n",
      "Epoch [214/400], Loss: 0.0008797955419868231\n",
      "Epoch [215/400], Loss: 0.0008787063881754876\n",
      "Epoch [216/400], Loss: 0.0008787041855975986\n",
      "Epoch [217/400], Loss: 0.0008799103205092251\n",
      "Epoch [218/400], Loss: 0.0008807676611468195\n",
      "Epoch [219/400], Loss: 0.0008746551861986518\n",
      "Epoch [220/400], Loss: 0.0008784913877025246\n",
      "Epoch [221/400], Loss: 0.0008838057587854564\n",
      "Epoch [222/400], Loss: 0.0008767848229035735\n",
      "Epoch [223/400], Loss: 0.0008780972938984633\n",
      "Epoch [224/400], Loss: 0.0008729322254657745\n",
      "Epoch [225/400], Loss: 0.0008725201175548136\n",
      "Epoch [226/400], Loss: 0.0008721674466505647\n",
      "Epoch [227/400], Loss: 0.0008720851480029523\n",
      "Epoch [228/400], Loss: 0.0008739941520616412\n",
      "Epoch [229/400], Loss: 0.0008692122367210686\n",
      "Epoch [230/400], Loss: 0.0008700660755857825\n",
      "Epoch [231/400], Loss: 0.0008701640483923256\n",
      "Epoch [232/400], Loss: 0.0008730280073359609\n",
      "Epoch [233/400], Loss: 0.0008715015975758433\n",
      "Epoch [234/400], Loss: 0.0008690341073088348\n",
      "Epoch [235/400], Loss: 0.0008685575053095818\n",
      "Epoch [236/400], Loss: 0.0008672144613228738\n",
      "Epoch [237/400], Loss: 0.0008705836650915444\n",
      "Epoch [238/400], Loss: 0.0008705552318133414\n",
      "Epoch [239/400], Loss: 0.0008660997776314617\n",
      "Epoch [240/400], Loss: 0.0008670935849659145\n",
      "Epoch [241/400], Loss: 0.0008755043940618635\n",
      "Epoch [242/400], Loss: 0.0008637843164615333\n",
      "Epoch [243/400], Loss: 0.0008621813217177987\n",
      "Epoch [244/400], Loss: 0.0008670842251740396\n",
      "Epoch [245/400], Loss: 0.0008674445725046098\n",
      "Epoch [246/400], Loss: 0.0008610261254943907\n",
      "Epoch [247/400], Loss: 0.0008628483791835606\n",
      "Epoch [248/400], Loss: 0.0008604898187331855\n",
      "Epoch [249/400], Loss: 0.0008637082297354937\n",
      "Epoch [250/400], Loss: 0.0008645198843441903\n",
      "Epoch [251/400], Loss: 0.0008568596281111241\n",
      "Epoch [252/400], Loss: 0.0008607189520262182\n",
      "Epoch [253/400], Loss: 0.0008592685288749635\n",
      "Epoch [254/400], Loss: 0.0008619112055748701\n",
      "Epoch [255/400], Loss: 0.0008567761117592454\n",
      "Epoch [256/400], Loss: 0.0008592939679510892\n",
      "Epoch [257/400], Loss: 0.0008659921283833683\n",
      "Epoch [258/400], Loss: 0.0008618483250029385\n",
      "Epoch [259/400], Loss: 0.0008543883892707527\n",
      "Epoch [260/400], Loss: 0.0008557606139220298\n",
      "Epoch [261/400], Loss: 0.000852658487856388\n",
      "Epoch [262/400], Loss: 0.0008544901991263032\n",
      "Epoch [263/400], Loss: 0.0008559886016882956\n",
      "Epoch [264/400], Loss: 0.0008584875054657459\n",
      "Epoch [265/400], Loss: 0.0008493122039362789\n",
      "Epoch [266/400], Loss: 0.0008551831543445588\n",
      "Epoch [267/400], Loss: 0.0008532162196934223\n",
      "Epoch [268/400], Loss: 0.0008552142255939543\n",
      "Epoch [269/400], Loss: 0.0008572884253226221\n",
      "Epoch [270/400], Loss: 0.0008517996594309807\n",
      "Epoch [271/400], Loss: 0.0008498602593317628\n",
      "Epoch [272/400], Loss: 0.0008491199975833297\n",
      "Epoch [273/400], Loss: 0.0008493930543772877\n",
      "Epoch [274/400], Loss: 0.0008475486538372934\n",
      "Epoch [275/400], Loss: 0.0008467562659643591\n",
      "Epoch [276/400], Loss: 0.0008531715092249215\n",
      "Epoch [277/400], Loss: 0.0008503076038323343\n",
      "Epoch [278/400], Loss: 0.000852513569407165\n",
      "Epoch [279/400], Loss: 0.0008564533037133515\n",
      "Epoch [280/400], Loss: 0.0008449046057648957\n",
      "Epoch [281/400], Loss: 0.0008440724550746381\n",
      "Epoch [282/400], Loss: 0.0008507417305372656\n",
      "Epoch [283/400], Loss: 0.000857685087248683\n",
      "Epoch [284/400], Loss: 0.0008480158587917685\n",
      "Epoch [285/400], Loss: 0.0008422001777216793\n",
      "Epoch [286/400], Loss: 0.0008417384163476527\n",
      "Epoch [287/400], Loss: 0.0008438674150966108\n",
      "Epoch [288/400], Loss: 0.000848584717605263\n",
      "Epoch [289/400], Loss: 0.000844312293920666\n",
      "Epoch [290/400], Loss: 0.0008396734716370702\n",
      "Epoch [291/400], Loss: 0.0008482258906587958\n",
      "Epoch [292/400], Loss: 0.0008419053652323782\n",
      "Epoch [293/400], Loss: 0.0008447774592787027\n",
      "Epoch [294/400], Loss: 0.0008395100152119994\n",
      "Epoch [295/400], Loss: 0.0008419403084553778\n",
      "Epoch [296/400], Loss: 0.0008386478666216135\n",
      "Epoch [297/400], Loss: 0.0008420062251389027\n",
      "Epoch [298/400], Loss: 0.0008362040878273547\n",
      "Epoch [299/400], Loss: 0.0008400006429292262\n",
      "Epoch [300/400], Loss: 0.0008421781053766608\n",
      "Epoch [301/400], Loss: 0.0008431715168990194\n",
      "Epoch [302/400], Loss: 0.0008507580123841762\n",
      "Epoch [303/400], Loss: 0.0008366781543008983\n",
      "Epoch [304/400], Loss: 0.000837929779663682\n",
      "Epoch [305/400], Loss: 0.0008357113832607865\n",
      "Epoch [306/400], Loss: 0.0008469926589168608\n",
      "Epoch [307/400], Loss: 0.0008414096012711525\n",
      "Epoch [308/400], Loss: 0.0008379163267090917\n",
      "Epoch [309/400], Loss: 0.0008348608203232288\n",
      "Epoch [310/400], Loss: 0.000833850719500333\n",
      "Epoch [311/400], Loss: 0.0008309127343818545\n",
      "Epoch [312/400], Loss: 0.0008376222453080117\n",
      "Epoch [313/400], Loss: 0.0008329713135026395\n",
      "Epoch [314/400], Loss: 0.0008353659301064909\n",
      "Epoch [315/400], Loss: 0.000836421144194901\n",
      "Epoch [316/400], Loss: 0.0008317042072303593\n",
      "Epoch [317/400], Loss: 0.000832739828620106\n",
      "Epoch [318/400], Loss: 0.0008306585578247904\n",
      "Epoch [319/400], Loss: 0.0008293142868205905\n",
      "Epoch [320/400], Loss: 0.0008298928709700704\n",
      "Epoch [321/400], Loss: 0.0008321195282042027\n",
      "Epoch [322/400], Loss: 0.0008312195865437388\n",
      "Epoch [323/400], Loss: 0.0008306546323001385\n",
      "Epoch [324/400], Loss: 0.0008274147799238562\n",
      "Epoch [325/400], Loss: 0.0008255585934966803\n",
      "Epoch [326/400], Loss: 0.0008304008282721042\n",
      "Epoch [327/400], Loss: 0.0008272409066557885\n",
      "Epoch [328/400], Loss: 0.0008293041260913015\n",
      "Epoch [329/400], Loss: 0.0008277267403900623\n",
      "Epoch [330/400], Loss: 0.0008259034901857376\n",
      "Epoch [331/400], Loss: 0.0008255105768330396\n",
      "Epoch [332/400], Loss: 0.0008273151936009527\n",
      "Epoch [333/400], Loss: 0.0008251220104284584\n",
      "Epoch [334/400], Loss: 0.0008244388783350587\n",
      "Epoch [335/400], Loss: 0.0008263713493943214\n",
      "Epoch [336/400], Loss: 0.0008239602064713836\n",
      "Epoch [337/400], Loss: 0.0008281452930532396\n",
      "Epoch [338/400], Loss: 0.0008233537920750677\n",
      "Epoch [339/400], Loss: 0.000821975264698267\n",
      "Epoch [340/400], Loss: 0.0008231181232258677\n",
      "Epoch [341/400], Loss: 0.0008225279045291245\n",
      "Epoch [342/400], Loss: 0.0008223428577184678\n",
      "Epoch [343/400], Loss: 0.0008226211927831173\n",
      "Epoch [344/400], Loss: 0.0008274492574855685\n",
      "Epoch [345/400], Loss: 0.0008237687102518976\n",
      "Epoch [346/400], Loss: 0.0008281776611693204\n",
      "Epoch [347/400], Loss: 0.0008230715920217335\n",
      "Epoch [348/400], Loss: 0.0008192581869661808\n",
      "Epoch [349/400], Loss: 0.0008248494635336102\n",
      "Epoch [350/400], Loss: 0.0008246774203144013\n",
      "Epoch [351/400], Loss: 0.0008188206097111106\n",
      "Epoch [352/400], Loss: 0.0008243168564513326\n",
      "Epoch [353/400], Loss: 0.0008198564033955335\n",
      "Epoch [354/400], Loss: 0.0008210096368566156\n",
      "Epoch [355/400], Loss: 0.0008156221336685121\n",
      "Epoch [356/400], Loss: 0.0008167347335256637\n",
      "Epoch [357/400], Loss: 0.0008203880721703172\n",
      "Epoch [358/400], Loss: 0.0008148287725634873\n",
      "Epoch [359/400], Loss: 0.0008196769678033888\n",
      "Epoch [360/400], Loss: 0.0008175148791633546\n",
      "Epoch [361/400], Loss: 0.0008170382538810373\n",
      "Epoch [362/400], Loss: 0.0008184085227549076\n",
      "Epoch [363/400], Loss: 0.0008169884397648275\n",
      "Epoch [364/400], Loss: 0.0008232063543982804\n",
      "Epoch [365/400], Loss: 0.0008151722862385214\n",
      "Epoch [366/400], Loss: 0.0008156263153068722\n",
      "Epoch [367/400], Loss: 0.0008152883453294634\n",
      "Epoch [368/400], Loss: 0.0008225810900330543\n",
      "Epoch [369/400], Loss: 0.0008149375463835895\n",
      "Epoch [370/400], Loss: 0.0008150541107170284\n",
      "Epoch [371/400], Loss: 0.0008132069767452777\n",
      "Epoch [372/400], Loss: 0.0008178329141810537\n",
      "Epoch [373/400], Loss: 0.0008109270269051194\n",
      "Epoch [374/400], Loss: 0.0008096280856989324\n",
      "Epoch [375/400], Loss: 0.0008160441392101348\n",
      "Epoch [376/400], Loss: 0.0008233828842639923\n",
      "Epoch [377/400], Loss: 0.000808808512520045\n",
      "Epoch [378/400], Loss: 0.0008092764718458056\n",
      "Epoch [379/400], Loss: 0.0008096763235516846\n",
      "Epoch [380/400], Loss: 0.000809863181784749\n",
      "Epoch [381/400], Loss: 0.0008118657977320253\n",
      "Epoch [382/400], Loss: 0.0008177436026744544\n",
      "Epoch [383/400], Loss: 0.0008137683081440628\n",
      "Epoch [384/400], Loss: 0.0008095037774182856\n",
      "Epoch [385/400], Loss: 0.0008094969717785716\n",
      "Epoch [386/400], Loss: 0.0008133606310002506\n",
      "Epoch [387/400], Loss: 0.0008080203621648252\n",
      "Epoch [388/400], Loss: 0.0008083948353305459\n",
      "Epoch [389/400], Loss: 0.000810110925231129\n",
      "Epoch [390/400], Loss: 0.0008103573997505009\n",
      "Epoch [391/400], Loss: 0.0008069235179573298\n",
      "Epoch [392/400], Loss: 0.0008078794833272695\n",
      "Epoch [393/400], Loss: 0.0008029947848990559\n",
      "Epoch [394/400], Loss: 0.0008093869895674288\n",
      "Epoch [395/400], Loss: 0.0008067384199239313\n",
      "Epoch [396/400], Loss: 0.0008015401172451675\n",
      "Epoch [397/400], Loss: 0.0008046691888011992\n",
      "Epoch [398/400], Loss: 0.0008065050723962486\n",
      "Epoch [399/400], Loss: 0.0008100698282942176\n",
      "Epoch [400/400], Loss: 0.0008138559269718825\n"
     ]
    }
   ],
   "source": [
    "# Training loop with batch processing\n",
    "epochs = 400\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0  # Reset running loss for each epoch\n",
    "    \n",
    "    # Process each batch\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        predictions = model(batch_x).squeeze()  # Remove singleton dimension\n",
    "        loss = criterion(predictions, batch_y)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()# Accumulate batch loss\n",
    "\n",
    "    # Calculate the average loss for the epoch\n",
    "    epoch_loss = (running_loss / len(train_loader))\n",
    "    \n",
    "    # Calculate accuracy and loss for validation set\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    with torch.no_grad():  # No need to track gradients during evaluation\n",
    "        y_val_pred = model(X_val_tensor).squeeze()\n",
    "        val_loss = criterion(y_val_pred, y_val_tensor).item()\n",
    "        val_mse = mean_squared_error(y_val_np, y_val_pred.numpy())\n",
    "        val_r2 = r2_score(y_val_np, y_val_pred.numpy())  # R² score\n",
    "        val_accuracy = val_r2 * 100  # Convert to percentage\n",
    "\n",
    "        # Store logs for this epoch\n",
    "        train_losses.append(epoch_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_accuracies.append(val_accuracy)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "\n",
    "    # Print results for each epoch\n",
    "    print(f'Epoch [{epoch + 1}/{epochs}], Loss: {epoch_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained model and logs\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'train_losses': train_losses,\n",
    "    'train_accuracies': train_accuracies,\n",
    "    'val_losses': val_losses,\n",
    "    'val_accuracies': val_accuracies\n",
    "}, 'cnn_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error on test data: 0.0008419479059511409\n",
      "R² score (Accuracy) on test data: 0.9785310755562355\n",
      "Accuracy: 97.85310755562355%\n"
     ]
    }
   ],
   "source": [
    "# Testing on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_test_pred = model(X_test_tensor).squeeze().numpy()\n",
    "    \n",
    "    # Calculate MSE and R² score\n",
    "    test_mse = mean_squared_error(y_test_np, y_test_pred)\n",
    "    test_r2 = r2_score(y_test_np, y_test_pred)  # R² score\n",
    "\n",
    "    # Calculate accuracy as R² score in percentage\n",
    "    test_accuracy = test_r2 * 100\n",
    "    \n",
    "    print(f'Mean Squared Error on test data: {test_mse}')\n",
    "    print(f'R² score (Accuracy) on test data: {test_r2}')\n",
    "    print(f'Accuracy: {test_accuracy}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
